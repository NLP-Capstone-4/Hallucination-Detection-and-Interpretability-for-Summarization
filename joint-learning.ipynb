{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d1816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re, json\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_metric,Dataset,DatasetDict\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2853ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('annotated_capstone_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8e2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.DictReader(data)\n",
    "myList = list()\n",
    "for dictionary in reader:\n",
    "    myList.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714689ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = []\n",
    "gold_sum = []\n",
    "generated_sum = []\n",
    "hal_tags = []\n",
    "\n",
    "for entry in myList[:100]:\n",
    "    dialogues.append(entry['Dialogue'].strip())\n",
    "    gold_sum.append(entry['Reference Summary'].strip())\n",
    "    generated_sum.append(entry['Generated Summary'].strip())\n",
    "    hal_tags.append(entry['Annotations'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85e5ba9-97ce-4ce3-9272-2788375603bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'dialogue': dialogues, 'summary':generated_sum, 'tags':hal_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e514f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_dict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f103613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = DatasetDict({'train':train_hf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38581099-7bd3-4b70-a18c-cb2113f907e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialogue', 'summary', 'tags'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37aecab9-478b-4f77-b975-a7b3d0e8014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc249f86-7815-4382-a0c1-b086894ac36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b579cfe1-ae90-4d0d-a5c2-3cfa64fe91f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pvakhari/miniconda3/envs/nlp_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sum = generated_sum[0]\n",
    "tags = hal_tags[0]\n",
    "with tokenizer.as_target_tokenizer():\n",
    "     labels = tokenizer(sum, max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d57242-97e0-433c-8851-431fdc587bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 34318, 75, 33, 16666, 18, 346, 4, 10641, 3649, 11029, 7, 1394, 6045, 4, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e9e6e02-b44b-4b26-a936-524b0cd6783e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'doesn',\n",
       " \"'t\",\n",
       " 'Ġhave',\n",
       " 'ĠBetty',\n",
       " \"'s\",\n",
       " 'Ġnumber',\n",
       " '.',\n",
       " 'ĠAmanda',\n",
       " 'Ġsuggests',\n",
       " 'ĠHannah',\n",
       " 'Ġto',\n",
       " 'Ġask',\n",
       " 'ĠLarry',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(labels['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d20f66be-35c1-4724-bf28-a36e0861fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(sum, tags):\n",
    "    print(sum)\n",
    "    tokenized_inputs = tokenizer(sum, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    # for i, label in enumerate(tags):\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=0)  # Map tokens to their respective word.\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "            label_ids.append(tags[word_idx])\n",
    "        else:\n",
    "            label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    print(label_ids)\n",
    "        # labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    # return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "278fb0ee-c3c8-4503-886d-c440630e6533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doesn't have Betty's number. Amanda suggests Hannah to ask Larry.\n",
      "[-100, 'O', ' ', 'O', ' ', 'O', ' ', 'O', ' ', 'O', ' ', 'O', ' ', 'O', ' ', -100]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_align_labels(sum, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c948cd5f-63b9-4e0f-ba13-bc47b9c94e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O O O O O O O O O O O O M'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5c1d969-79e4-4b5f-9409-86ee43e2badd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'doesn',\n",
       " \"'t\",\n",
       " 'Ġhave',\n",
       " 'ĠBetty',\n",
       " \"'s\",\n",
       " 'Ġnumber',\n",
       " '.',\n",
       " 'ĠAmanda',\n",
       " 'Ġsuggests',\n",
       " 'ĠHannah',\n",
       " 'Ġto',\n",
       " 'Ġask',\n",
       " 'ĠLarry',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a777c-7b57-429c-839d-69523ba9632d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
