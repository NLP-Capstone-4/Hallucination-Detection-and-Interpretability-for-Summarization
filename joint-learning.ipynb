{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d1816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re, json\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_metric,Dataset,DatasetDict\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2853ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('annotated_capstone_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8e2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.DictReader(data)\n",
    "myList = list()\n",
    "for dictionary in reader:\n",
    "    myList.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714689ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = []\n",
    "gold_sum = []\n",
    "generated_sum = []\n",
    "hal_tags = []\n",
    "\n",
    "for entry in myList[:100]:\n",
    "    dialogues.append(entry['Dialogue'].strip())\n",
    "    gold_sum.append(entry['Reference Summary'].strip())\n",
    "    generated_sum.append(entry['Generated Summary'].strip())\n",
    "    hal_tags.append(entry['Annotations'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e85e5ba9-97ce-4ce3-9272-2788375603bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'dialogue': dialogues, 'summary':generated_sum, 'tags':hal_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e514f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_dict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f103613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = DatasetDict({'train':train_hf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38581099-7bd3-4b70-a18c-cb2113f907e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialogue', 'summary', 'tags'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37aecab9-478b-4f77-b975-a7b3d0e8014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc249f86-7815-4382-a0c1-b086894ac36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.63k/1.63k [00:00<00:00, 4.41MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.02G/1.02G [02:51<00:00, 5.94MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 128kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.88MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.13MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b579cfe1-ae90-4d0d-a5c2-3cfa64fe91f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parsamz/Canvas/Capstone/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3848: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sum = generated_sum[1]\n",
    "tags = hal_tags[1].split(' ')\n",
    "with tokenizer.as_target_tokenizer():\n",
    "     labels = tokenizer(sum, max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ddc769-2472-48d9-aa83-1523613aa303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'W', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d9ea205-aec9-4103-9c6d-832e874ffc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda can't find Betty's number. Larry called her last time they were at the park together. Amanda will text Larry.\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'ĠAmanda', '</s>']\n",
      "['<s>', 'ĠAmanda', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'Ġcan', \"'t\", '</s>']\n",
      "['<s>', 'Ġcan', \"'t\", '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġfind', '</s>']\n",
      "['<s>', 'Ġfind', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'ĠBetty', \"'s\", '</s>']\n",
      "['<s>', 'ĠBetty', \"'s\", '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'Ġnumber', '.', '</s>']\n",
      "['<s>', 'Ġnumber', '.', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'ĠLarry', '</s>']\n",
      "['<s>', 'ĠLarry', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġcalled', '</s>']\n",
      "['<s>', 'Ġcalled', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġher', '</s>']\n",
      "['<s>', 'Ġher', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġlast', '</s>']\n",
      "['<s>', 'Ġlast', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġtime', '</s>']\n",
      "['<s>', 'Ġtime', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġthey', '</s>']\n",
      "['<s>', 'Ġthey', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġwere', '</s>']\n",
      "['<s>', 'Ġwere', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġat', '</s>']\n",
      "['<s>', 'Ġat', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġthe', '</s>']\n",
      "['<s>', 'Ġthe', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġpark', '</s>']\n",
      "['<s>', 'Ġpark', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'Ġtogether', '.', '</s>']\n",
      "['<s>', 'Ġtogether', '.', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'ĠAmanda', '</s>']\n",
      "['<s>', 'ĠAmanda', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġwill', '</s>']\n",
      "['<s>', 'Ġwill', '</s>']\n",
      "W\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġtext', '</s>']\n",
      "['<s>', 'Ġtext', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'ĠLarry', '.', '</s>']\n",
      "['<s>', 'ĠLarry', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(sum)\n",
    "split_tags = [None]\n",
    "final = ['<s>']\n",
    "for i, word in enumerate(sum.split(' ')):\n",
    "    encoded = tokenizer([word], is_split_into_words=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "    length_of_subtokens = len(tokens[1:-1])\n",
    "    print(tags[i])\n",
    "    print(\"length_of_subtokens: \", length_of_subtokens)\n",
    "    print(\"tokens: \",  tokens)\n",
    "    if length_of_subtokens >= 1:\n",
    "        split_tags.append(tags[i])\n",
    "        length_of_subtokens-=1\n",
    "        while length_of_subtokens >= 1:\n",
    "            split_tags.append('-100')\n",
    "            length_of_subtokens-=1            \n",
    "    \n",
    "    final.extend(tokens[1:-1])\n",
    "    print(tokens)\n",
    "final.append('</s>')\n",
    "split_tags.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0956ea77-77e8-4f83-a5b3-4551b0525863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'O', 'O', '-100', 'O', 'O', '-100', 'O', '-100', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-100', 'O', 'O', 'W', 'O', '-100', None]\n"
     ]
    }
   ],
   "source": [
    "print(split_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f08e25b-af8b-41b1-aacf-3096fd16c5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'ĠAmanda', 'Ġcan', \"'t\", 'Ġfind', 'ĠBetty', \"'s\", 'Ġnumber', '.', 'ĠLarry', 'Ġcalled', 'Ġher', 'Ġlast', 'Ġtime', 'Ġthey', 'Ġwere', 'Ġat', 'Ġthe', 'Ġpark', 'Ġtogether', '.', 'ĠAmanda', 'Ġwill', 'Ġtext', 'ĠLarry', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f66be-35c1-4724-bf28-a36e0861fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_align_labels(tokens, tags):\n",
    "#     print(tokens)\n",
    "#     print(tags)\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         tokenized_inputs = tokenizer(tokens, max_length=128, truncation=True)\n",
    "\n",
    "#     labels = []\n",
    "#     # for i, label in enumerate(tags):\n",
    "#     word_ids = tokenized_inputs.word_ids()  # Map tokens to their respective word.\n",
    "#     print(word_ids)\n",
    "#     previous_word_idx = None\n",
    "#     label_ids = []\n",
    "#     for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "#         if word_idx is None:\n",
    "#             label_ids.append(-100)\n",
    "#         elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "#             label_ids.append(tags[word_idx])\n",
    "#         else:\n",
    "#             label_ids.append(-100)\n",
    "#         previous_word_idx = word_idx\n",
    "#     print(label_ids)\n",
    "#         # labels.append(label_ids)\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     # return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc50221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "class ExtendedBartModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        num_tags = 6\n",
    "        super(ExtendedBartModel, self).__init__()\n",
    "        self.bart = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        # self.token_tagging_layer = nn.Linear(self.bart.config.d_model, num_tags)  \n",
    "        # Assuming num_tags is the number of possible tags\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        bart_outputs = self.bart(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        print(\"output: \", bart_outputs)\n",
    "        # token_tags_logits = self.token_tagging_layer(bart_outputs)\n",
    "        return bart_outputs  \n",
    "    \n",
    "\n",
    "    def generate(self, input_ids, attention_mask=None):\n",
    "        bart_output = self.bart.generate(input_ids, attention_mask=attention_mask)\n",
    "        print(\"bart out: \", bart_output)\n",
    "        # tag \n",
    "        return bart_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "97753946",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large\"\n",
    "model = ExtendedBartModel(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# freeze the weights of the BART model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72b38dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda can't find Betty's number. Larry called her last time they were at the park together. Amanda will text Larry.\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'W', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "summary = generated_sum[1]\n",
    "tags = hal_tags[1].split(' ')\n",
    "\n",
    "print(summary)\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c60e583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': \"Will: hey babe, what do you want for dinner tonight?\\nEmma:  gah, don't even worry about it tonight\\nWill: what do you mean? everything ok?\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\nWill: Well what time will you be home?\\nEmma: soon, hopefully\\nWill: you sure? Maybe you want me to pick you up?\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home. \\nWill: Alright, love you. \\nEmma: love you too.\",\n",
       " 'summary': 'Will will pick Emma up when he gets home.',\n",
       " 'tags': 'W O C W C O W O O O'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hf[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0fc5d8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parsamz/Canvas/Capstone/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  tensor([[    2,     0,     0,     0, 16750,  1916,    18,  1028, 11987,     4,\n",
      "          2290,    35, 17232, 37502,     6,    99,   109,    47,   236,     2]])\n",
      "Emma's phone rings. Will: hey babe, what do you want\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(\n",
    "    train_hf[21]['dialogue'], return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
