{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d1816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re, json\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_metric,Dataset,DatasetDict\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2853ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('annotated_capstone_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8e2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.DictReader(data)\n",
    "myList = list()\n",
    "for dictionary in reader:\n",
    "    myList.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714689ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = []\n",
    "gold_sum = []\n",
    "generated_sum = []\n",
    "hal_tags = []\n",
    "\n",
    "for entry in myList[:100]:\n",
    "    dialogues.append(entry['Dialogue'].strip())\n",
    "    gold_sum.append(entry['Reference Summary'].strip())\n",
    "    generated_sum.append(entry['Generated Summary'].strip())\n",
    "    hal_tags.append(entry['Annotations'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85e5ba9-97ce-4ce3-9272-2788375603bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'dialogue': dialogues, 'summary':generated_sum, 'tags':hal_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e514f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_dict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f103613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = DatasetDict({'train':train_hf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38581099-7bd3-4b70-a18c-cb2113f907e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialogue', 'summary', 'tags'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37aecab9-478b-4f77-b975-a7b3d0e8014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc249f86-7815-4382-a0c1-b086894ac36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b579cfe1-ae90-4d0d-a5c2-3cfa64fe91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = generated_sum[1]\n",
    "tags = hal_tags[1].split(' ')\n",
    "with tokenizer.as_target_tokenizer():\n",
    "     labels = tokenizer(sum, max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8ddc769-2472-48d9-aa83-1523613aa303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'W',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4d9ea205-aec9-4103-9c6d-832e874ffc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda can't find Betty's number. Larry called her last time they were at the park together. Amanda will text Larry.\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'ĠAmanda', '</s>']\n",
      "['<s>', 'ĠAmanda', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'Ġcan', \"'t\", '</s>']\n",
      "['<s>', 'Ġcan', \"'t\", '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġfind', '</s>']\n",
      "['<s>', 'Ġfind', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'ĠBetty', \"'s\", '</s>']\n",
      "['<s>', 'ĠBetty', \"'s\", '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'Ġnumber', '.', '</s>']\n",
      "['<s>', 'Ġnumber', '.', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'ĠLarry', '</s>']\n",
      "['<s>', 'ĠLarry', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġcalled', '</s>']\n",
      "['<s>', 'Ġcalled', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġher', '</s>']\n",
      "['<s>', 'Ġher', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġlast', '</s>']\n",
      "['<s>', 'Ġlast', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġtime', '</s>']\n",
      "['<s>', 'Ġtime', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġthey', '</s>']\n",
      "['<s>', 'Ġthey', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġwere', '</s>']\n",
      "['<s>', 'Ġwere', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġat', '</s>']\n",
      "['<s>', 'Ġat', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġthe', '</s>']\n",
      "['<s>', 'Ġthe', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġpark', '</s>']\n",
      "['<s>', 'Ġpark', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'Ġtogether', '.', '</s>']\n",
      "['<s>', 'Ġtogether', '.', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'ĠAmanda', '</s>']\n",
      "['<s>', 'ĠAmanda', '</s>']\n",
      "O\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġwill', '</s>']\n",
      "['<s>', 'Ġwill', '</s>']\n",
      "W\n",
      "length_of_subtokens:  1\n",
      "tokens:  ['<s>', 'Ġtext', '</s>']\n",
      "['<s>', 'Ġtext', '</s>']\n",
      "O\n",
      "length_of_subtokens:  2\n",
      "tokens:  ['<s>', 'ĠLarry', '.', '</s>']\n",
      "['<s>', 'ĠLarry', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(sum)\n",
    "split_tags = [None]\n",
    "final = ['<s>']\n",
    "for i, word in enumerate(sum.split(' ')):\n",
    "    encoded = tokenizer([word], is_split_into_words=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "    length_of_subtokens = len(tokens[1:-1])\n",
    "    print(tags[i])\n",
    "    print(\"length_of_subtokens: \", length_of_subtokens)\n",
    "    print(\"tokens: \",  tokens)\n",
    "    if length_of_subtokens >= 1:\n",
    "        split_tags.append(tags[i])\n",
    "        length_of_subtokens-=1\n",
    "        while length_of_subtokens >= 1:\n",
    "            split_tags.append('-100')\n",
    "            length_of_subtokens-=1            \n",
    "    \n",
    "    final.extend(tokens[1:-1])\n",
    "    print(tokens)\n",
    "final.append('</s>')\n",
    "split_tags.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0956ea77-77e8-4f83-a5b3-4551b0525863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'O',\n",
       " 'O',\n",
       " '-100',\n",
       " 'O',\n",
       " 'O',\n",
       " '-100',\n",
       " 'O',\n",
       " '-100',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " '-100',\n",
       " 'O',\n",
       " 'O',\n",
       " 'W',\n",
       " 'O',\n",
       " '-100',\n",
       " None]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3f08e25b-af8b-41b1-aacf-3096fd16c5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'ĠAmanda',\n",
       " 'Ġcan',\n",
       " \"'t\",\n",
       " 'Ġfind',\n",
       " 'ĠBetty',\n",
       " \"'s\",\n",
       " 'Ġnumber',\n",
       " '.',\n",
       " 'ĠLarry',\n",
       " 'Ġcalled',\n",
       " 'Ġher',\n",
       " 'Ġlast',\n",
       " 'Ġtime',\n",
       " 'Ġthey',\n",
       " 'Ġwere',\n",
       " 'Ġat',\n",
       " 'Ġthe',\n",
       " 'Ġpark',\n",
       " 'Ġtogether',\n",
       " '.',\n",
       " 'ĠAmanda',\n",
       " 'Ġwill',\n",
       " 'Ġtext',\n",
       " 'ĠLarry',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d07175d-aa40-4fae-848c-53e991fdcceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O O -100 O O -100 O -100 O O O O O O O O O O O -100 W O O O -100'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(split_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20f66be-35c1-4724-bf28-a36e0861fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_align_labels(tokens, tags):\n",
    "#     print(tokens)\n",
    "#     print(tags)\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         tokenized_inputs = tokenizer(tokens, max_length=128, truncation=True)\n",
    "\n",
    "#     labels = []\n",
    "#     # for i, label in enumerate(tags):\n",
    "#     word_ids = tokenized_inputs.word_ids()  # Map tokens to their respective word.\n",
    "#     print(word_ids)\n",
    "#     previous_word_idx = None\n",
    "#     label_ids = []\n",
    "#     for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "#         if word_idx is None:\n",
    "#             label_ids.append(-100)\n",
    "#         elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "#             label_ids.append(tags[word_idx])\n",
    "#         else:\n",
    "#             label_ids.append(-100)\n",
    "#         previous_word_idx = word_idx\n",
    "#     print(label_ids)\n",
    "#         # labels.append(label_ids)\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     # return tokenized_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
