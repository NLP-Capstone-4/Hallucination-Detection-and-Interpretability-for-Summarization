{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jare6MPF3DcI",
    "outputId": "b68b3d4e-dfeb-468f-8b6f-ff7204db30ef"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install langchain\n",
    "#!pip install openai\n",
    "#!pip install backoff\n",
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gifN-0wMF7Yz"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def convert_to_iob(d):\n",
    "    for i in range(len(d)):\n",
    "        for j in range(len(d[i])):\n",
    "            if d[i][j] != 'O':\n",
    "                d[i][j] = 'B-' + d[i][j]\n",
    "    return d\n",
    "\n",
    "\n",
    "def compute_metrics_seqeval(gold, predictions):\n",
    "    true_labels_iob = convert_to_iob(gold)\n",
    "    true_predictions_iob = convert_to_iob(predictions)\n",
    "\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions_iob, references=true_labels_iob)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519,
     "referenced_widgets": [
      "aff2e5e491db40e7a875c96d746e539c",
      "1c70328f340548f6aedcc0d4e0780e0b",
      "cefd81d7e4a9429bbe13fae45cdc7db3",
      "16c28be2234545e3b2207641be5ea8c5",
      "b2e28bf8074c43e8a00865bb418fb023",
      "c67230e5982c475b997875665729ce12",
      "a44df117298e4e0a90a31d9c865a7ab1",
      "837d7ae5807948218deaf6263dcb494a",
      "c99280649f6c4e4a8b9a99db97b6939f",
      "b99d42dc86d54b6b9195ce7e927c42b6",
      "987b85416ab347368cc55b7fd16ce2ad"
     ]
    },
    "id": "ybaMrJsAt1I_",
    "outputId": "e09cc69d-826e-4fb5-fbb5-bb0ae1efe57d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['dialog_id', 'dialogue', 'summary', 'gold_tags'],\n",
      "        num_rows: 76\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['dialog_id', 'dialogue', 'summary', 'gold_tags'],\n",
      "        num_rows: 12\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['dialog_id', 'dialogue', 'summary', 'gold_tags'],\n",
      "        num_rows: 12\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import json\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "import openai\n",
    "import backoff  # for exponential backoff\n",
    "\n",
    "TAGS_BEGIN = \"Tags- <TG>\"\n",
    "TAGS_END = '<TG>'\n",
    "\n",
    "MI_BEGIN = \"Missing Information- <MI>\"\n",
    "MI_END = \"<MI>\"\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def get_dialog_turns(chat, query_text):\n",
    "    dialog_response= {}\n",
    "    dialog_response['Error'] = ''\n",
    "    try:\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=example1),\n",
    "            AIMessage(content=answer1),\n",
    "            HumanMessage(content=example2),\n",
    "            AIMessage(content=answer2),\n",
    "            HumanMessage(content=example3),\n",
    "            AIMessage(content=answer3),\n",
    "            HumanMessage(content=query_text)\n",
    "        ]\n",
    "\n",
    "        dialog_message = chat(messages)\n",
    "        dialog_response['text'] = dialog_message.content\n",
    "        dialog_response['success'] = True\n",
    "    except Exception as e:\n",
    "        dialog_response['text'] = ''\n",
    "        dialog_response['success'] = False\n",
    "        dialog_response['Error'] = str(e)\n",
    "\n",
    "    return dialog_response\n",
    "\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] =  \"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\"\n",
    "\n",
    "def get_dialog_query(record):\n",
    "    query_text = 'Dialogue- \"' + record['dialogue'] + '\"'\n",
    "    query_text += '\\n Summary- \"' + record['summary'] + '\"'\n",
    "    return query_text\n",
    "\n",
    "ds = load_dataset('Deojoandco/capstone_forgpt_without_gold')\n",
    "print(ds)\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4', temperature=0.0, max_tokens=2048)\n",
    "\n",
    "system_prompt = '''\n",
    "Given a set of dialogues and its summary, the first task is to do token-level classification. Analyze each token in the summary (not the meaning of the entire sentence or phrase) and label each token based on following guidelines:\n",
    "O = Not Hallucinated\n",
    "W =  Wrong person reference, only applies to tokens mentioning humans not present in the dialogue or in cases where the actions taken in the summary sentence are as per the dialogue but by the wrong human.\n",
    "C = Circumstancial error, applies in the predicate of a sentence when events or facts mentioned in the summary are completely wrong as they never were mentioned in the dialogue.\n",
    "OB = Object error, only applies to inanimate objects incorrectly mentioned in the summary because a different object is mentioned in the dialogue for similar context.\n",
    "N = uncommon errors like tense errors \n",
    "\n",
    "Once the token-level classification is done, the second task is to determine whether there is any important information from the dialogue missing in the summary. Answer 'yes' if there is any missing information else 'No'. \n",
    "'''\n",
    "\n",
    "example1 = '''\n",
    "Dialogue- \"Mary: hey, im kinda broke, lend me a few box\n",
    "Carter: okay, give me an hour, im at the train station\n",
    "Mary: cool, thanks\"\n",
    "\n",
    "Summary- \"Adam will lend Mary a box.\"\n",
    "'''\n",
    "\n",
    "answer1= '''\n",
    "Tags- <TG>Adam(W) will(O) lend(O) Mary(O) a(O) box(OB) .(O)<TG>\n",
    "\n",
    "There is important missing information that Carter will need another 1 hour to reach and lend money. Hence \"Yes\" for the missing information\n",
    "\n",
    "Missing Information- <MI>Yes<MI>\n",
    "'''\n",
    "\n",
    "example2 = '''\n",
    "Dialogue- \"Ernest: hey Mike, did you park your car on our street?\n",
    "Mike: no, took it into garage today\n",
    "Ernest: ok good\n",
    "Mike: why?\n",
    "Ernest: someone just crashed into a red honda looking just like yours\n",
    "Mike: lol lucky me\"\n",
    "\n",
    "Summary- \"Mike took his car to the garage today because it had been hit by another car.\"\n",
    "'''\n",
    "answer2='''\n",
    "Tags- <TG>Mike(O) took(O) his(O) car(O) to(O) the(O) garage(O) today(O) because(C) it(OB) had(N) been(N) hit(O) by(O) another(O) car(O) .(O)<TG>\n",
    "\n",
    "There is important missing information that Ernst is relieved as Mike's car is ok. Hence \"Yes\" for the missing information\n",
    "\n",
    "Missing Information- <MI>Yes<MI>\n",
    "'''\n",
    "\n",
    "example3 = '''\n",
    "Dialogue- \"Anne: You were right, he was lying to me :/\n",
    "Irene: Oh no, what happened?\n",
    "Jane: who? that Mark guy?\n",
    "Anne: yeah, he told me he's 30, today I saw his passport - he's 40\n",
    "Irene: You sure it's so important?\n",
    "Anne: he lied to me Irene\"\n",
    "\n",
    "Summary- \"Mark lied to Anne about his age. He's 40 now.\"\n",
    "'''\n",
    "\n",
    "answer3='''\n",
    "Tags- <TG>Mark(O) lied(O) to(O) Anne(O) about(O) his(O) age(O) .(O) He's(O) 40(O) now(O) .(O)<TG>\n",
    "\n",
    "There is no important missing information in the summary. Hence \"No\" for the missing information.\n",
    "\n",
    "Missing Information- <MI>No<MI>\n",
    "'''\n",
    "\n",
    "def run_gpt(ds, split):\n",
    "    records = []\n",
    "    GPT_SUCCESS = 0\n",
    "    GPT_MI_FOUND = 0\n",
    "    GPT_TAGS_FOUND = 0\n",
    "    token_count_matches = 0\n",
    "\n",
    "    pbar = tqdm_notebook(ds, desc=f'Processing {split}')\n",
    "    for i, record in enumerate(pbar):\n",
    "        query_text = get_dialog_query(record)\n",
    "\n",
    "        dialog_response = get_dialog_turns(chat, query_text)\n",
    "\n",
    "        #record['query'] = query_text\n",
    "        record['gpt_success'] = dialog_response['success']\n",
    "        record['gpt_response'] = dialog_response['text']\n",
    "\n",
    "        gold_tag_tokens = record['gold_tags'].split()\n",
    "        record['gold_tags_tokens_count'] = len(gold_tag_tokens)\n",
    "\n",
    "        if dialog_response['success'] == True:\n",
    "            GPT_SUCCESS += 1\n",
    "\n",
    "            output_tokens= []\n",
    "            output_tags = []\n",
    "            tags_start_index = record['gpt_response'].find(TAGS_BEGIN) + len(TAGS_BEGIN)\n",
    "            tags_end_index = record['gpt_response'].find(TAGS_END, tags_start_index)\n",
    "            if tags_start_index != -1 and tags_end_index != -1:\n",
    "                record['GPT_TAGS_FOUND'] = True\n",
    "                GPT_TAGS_FOUND += 1\n",
    "\n",
    "                record['gpt_output_tags'] = record['gpt_response'][tags_start_index:tags_end_index]\n",
    "\n",
    "                output_tag_tokens = record['gpt_output_tags'].split()\n",
    "                record['gpt_output_tag_tokens_count'] = len(output_tag_tokens)\n",
    "\n",
    "                for output_tag_token in output_tag_tokens:\n",
    "                    tag_tokens = output_tag_token.split('(')\n",
    "                    output_tokens.append(tag_tokens[0])\n",
    "                    if len(tag_tokens) == 2:\n",
    "                        output_tags.append(tag_tokens[1][:-1])\n",
    "\n",
    "            mi_start_index = record['gpt_response'].find(MI_BEGIN) + len(MI_BEGIN)\n",
    "            mi_end_index = record['gpt_response'].find(MI_END, mi_start_index)\n",
    "\n",
    "            if mi_start_index != -1 and mi_end_index != -1:\n",
    "                GPT_MI_FOUND += 1\n",
    "                record['GPT_MI_FOUND'] = True\n",
    "\n",
    "                mi_response = record['gpt_response'][mi_start_index:mi_end_index]\n",
    "\n",
    "                if mi_response.lower() == \"yes\":\n",
    "                    output_tags.append(\"M\")\n",
    "                else:\n",
    "                    output_tags.append(\"O\")\n",
    "\n",
    "\n",
    "            record['gpt_tags_token_count'] = len(output_tags)\n",
    "            record['gpt_tags'] = ' '.join(output_tags)\n",
    "\n",
    "            record['tag_token_count_match'] = record['gold_tags_tokens_count'] == record['gpt_tags_token_count']\n",
    "            if record['tag_token_count_match'] == True:\n",
    "                token_count_matches += 1\n",
    "        else:\n",
    "          record['GPT_TAGS_FOUND'] = False\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(data=records)\n",
    "    \n",
    "    print('-'*89)\n",
    "    print(f'stats for split: {split}')\n",
    "    print(f'GPT success Count: {GPT_SUCCESS}, Percentage: {GPT_SUCCESS * 100/len(records)}')\n",
    "\n",
    "    print(f'GPT Tags Found: {GPT_TAGS_FOUND}, Percentage: {GPT_TAGS_FOUND * 100/len(records)}')\n",
    "    print(f'GPT Missing Information Found: {GPT_MI_FOUND}, Percentage: {GPT_MI_FOUND * 100/len(records)}')\n",
    "\n",
    "    print(f'Gold and Prediction token length matching. Count: {token_count_matches}, Percentage: {token_count_matches * 100 / len(records)}')\n",
    "\n",
    "    length_matching_golds = []\n",
    "    length_matching_preds = []\n",
    "    for record in records:\n",
    "        if record['tag_token_count_match'] == True:\n",
    "            length_matching_golds.append(record['gold_tags'].split())\n",
    "            length_matching_preds.append(record['gpt_tags'].split())\n",
    "\n",
    "    score = compute_metrics_seqeval(length_matching_golds, length_matching_preds)\n",
    "    print(score)\n",
    "    \n",
    "    ds = datasets.Dataset.from_pandas(df, split=split)\n",
    "\n",
    "    # push splits to huggingface repo\n",
    "    print(\"Uploading to Huggingface\")\n",
    "    ds.push_to_hub('capstone_fromgpt_without_gold_v9_all', token ='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')\n",
    "    ds.to_csv(f'capstone_fromgpt_without_gold_v9_{split}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rirOQauxHvzT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178dc370eb7a489db5974ecc7c7a59c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "stats for split: train\n",
      "GPT success Count: 76, Percentage: 100.0\n",
      "GPT Tags Found: 76, Percentage: 100.0\n",
      "GPT Missing Information Found: 76, Percentage: 100.0\n",
      "Gold and Prediction token length matching. Count: 70, Percentage: 92.10526315789474\n",
      "{'precision': 0.6611570247933884, 'recall': 0.6349206349206349, 'f1': 0.6477732793522267, 'accuracy': 0.9525462962962963}\n",
      "Uploading to Huggingface\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a15c4a705c4b489e375924b3c083d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e2c80d340c4b01917f4f05088b6b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e89cf304cc84844bc7b2613527e1feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a08207de4946e98fbf2761cf03e872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "stats for split: validation\n",
      "GPT success Count: 12, Percentage: 100.0\n",
      "GPT Tags Found: 12, Percentage: 100.0\n",
      "GPT Missing Information Found: 12, Percentage: 100.0\n",
      "Gold and Prediction token length matching. Count: 12, Percentage: 100.0\n",
      "{'precision': 0.5490196078431373, 'recall': 0.6363636363636364, 'f1': 0.5894736842105264, 'accuracy': 0.8817891373801917}\n",
      "Uploading to Huggingface\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7198f6a0a314dcea9e038bbe4c7880d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22312ae34be543bfb2e57b35fab2d1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28db87b24b14fecaa030ef9852d841d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddea1397d2044c069392648087885651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2d662d3ae24eeeb8256ee612a8ade6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0232fb2d07db463b91b3b0d7b38dea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "stats for split: test\n",
      "GPT success Count: 12, Percentage: 100.0\n",
      "GPT Tags Found: 12, Percentage: 100.0\n",
      "GPT Missing Information Found: 12, Percentage: 100.0\n",
      "Gold and Prediction token length matching. Count: 12, Percentage: 100.0\n",
      "{'precision': 0.5625, 'recall': 0.8181818181818182, 'f1': 0.6666666666666666, 'accuracy': 0.9540816326530612}\n",
      "Uploading to Huggingface\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158880d976ab4608b94b69d2cd9a0c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b606c9177c4842bcc522bc1f4872fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6dee7c4609483989c78fc1d3a52477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c74bfe7978b4665aafe835928800618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7688f84e504871bb5ddc0d717161e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_gpt(ds['train'], 'train')\n",
    "run_gpt(ds['validation'], 'validation')\n",
    "run_gpt(ds['test'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16c28be2234545e3b2207641be5ea8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b99d42dc86d54b6b9195ce7e927c42b6",
      "placeholder": "​",
      "style": "IPY_MODEL_987b85416ab347368cc55b7fd16ce2ad",
      "value": " 35/100 [1:41:30&lt;4:09:52, 230.66s/it]"
     }
    },
    "1c70328f340548f6aedcc0d4e0780e0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c67230e5982c475b997875665729ce12",
      "placeholder": "​",
      "style": "IPY_MODEL_a44df117298e4e0a90a31d9c865a7ab1",
      "value": "Creating Dialog:  35%"
     }
    },
    "837d7ae5807948218deaf6263dcb494a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "987b85416ab347368cc55b7fd16ce2ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a44df117298e4e0a90a31d9c865a7ab1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aff2e5e491db40e7a875c96d746e539c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c70328f340548f6aedcc0d4e0780e0b",
       "IPY_MODEL_cefd81d7e4a9429bbe13fae45cdc7db3",
       "IPY_MODEL_16c28be2234545e3b2207641be5ea8c5"
      ],
      "layout": "IPY_MODEL_b2e28bf8074c43e8a00865bb418fb023"
     }
    },
    "b2e28bf8074c43e8a00865bb418fb023": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b99d42dc86d54b6b9195ce7e927c42b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c67230e5982c475b997875665729ce12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c99280649f6c4e4a8b9a99db97b6939f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cefd81d7e4a9429bbe13fae45cdc7db3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_837d7ae5807948218deaf6263dcb494a",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c99280649f6c4e4a8b9a99db97b6939f",
      "value": 35
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
