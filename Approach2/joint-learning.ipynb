{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1dcee7c-0363-40b2-8a67-1cae64898c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re, json\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_metric,Dataset,DatasetDict, load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, BartForConditionalGeneration\n",
    "from transformers import AutoTokenizer, Trainer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "from jointbart import myBartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899f8323-f0cd-4591-a7fc-3d0d37aca400",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7214fe4b-67d9-4614-8635-bb95efe58623",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48ae316-d6f6-40fa-8bb3-378e03d1eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_iob(d):\n",
    "    for i in range(len(d)):\n",
    "        for j in range(len(d[i])):\n",
    "            if d[i][j] != 'O':\n",
    "                d[i][j] = 'B-' + d[i][j]\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150af1a2-0059-44ef-ae69-7563650a6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large\"\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ae69bd-d3eb-4e28-b10a-3d30d3962e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7781f554-4604-4ff1-b9b2-fc5a998bc09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of myBartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = myBartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d9e4db0-eac9-42a8-94ad-7df8e88e2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('pvisnrt/capstone_hal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "376eef20-c4b0-44f4-8810-0622bc39cd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'summary_target', 'tags'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'summary_target', 'tags'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'summary_target', 'tags'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e73fb4-5b3e-4ae7-b1d6-83f5835e386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    inputs = [doc for doc in examples['source']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, is_split_into_words=True, return_tensors='pt', padding=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_inputs = tokenizer(examples[\"summary_target\"], truncation=True, is_split_into_words=True, return_tensors='pt', padding=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)# Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            \n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    model_inputs['labels'] = tokenized_inputs['input_ids']\n",
    "\n",
    "    model_inputs[\"decoder_tags\"] = labels\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42855419-2f1d-4c20-bd68-ab82d2c6f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2462a5d2-6415-4342-811d-765089e25a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_tags'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_tags'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'decoder_tags'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(['source','summary_target', 'tags'])\n",
    "tokenized_datasets['validation'] = tokenized_datasets['validation'].remove_columns(['source','summary_target', 'tags'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(['source','summary_target', 'tags'])\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82be15b0-cb84-4fc6-bcfb-d6936c742083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['validation']['decoder_tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ee65ba8-4816-447e-8e2c-8767959e4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "        Return:\n",
    "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n",
    "            labels (each being optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.args.predict_with_generate or prediction_loss_only:\n",
    "            return super().prediction_step(\n",
    "                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n",
    "            )\n",
    "\n",
    "        has_labels = \"decoder_tags\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        # print(\"prediction_step inputs: {}\".format(inputs.keys()))\n",
    "\n",
    "        # XXX: adapt synced_gpus for fairscale as well\n",
    "        gen_kwargs = self._gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
    "            gen_kwargs[\"max_length\"] = self.model.config.max_length\n",
    "        gen_kwargs[\"num_beams\"] = (\n",
    "            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.model.config.num_beams\n",
    "        )\n",
    "        # default_synced_gpus = True if is_deepspeed_zero3_enabled() else False\n",
    "        default_synced_gpus = False\n",
    "        gen_kwargs[\"synced_gpus\"] = (\n",
    "            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n",
    "        )\n",
    "\n",
    "        if \"attention_mask\" in inputs:\n",
    "            gen_kwargs[\"attention_mask\"] = inputs.get(\"attention_mask\", None)\n",
    "        if \"global_attention_mask\" in inputs:\n",
    "            gen_kwargs[\"global_attention_mask\"] = inputs.get(\"global_attention_mask\", None)\n",
    "\n",
    "        # prepare generation inputs\n",
    "        # some encoder-decoder models can have varying encoder's and thus\n",
    "        # varying model input names\n",
    "        if hasattr(self.model, \"encoder\") and self.model.encoder.main_input_name != self.model.main_input_name:\n",
    "            generation_inputs = inputs[self.model.encoder.main_input_name]\n",
    "        else:\n",
    "            generation_inputs = inputs[self.model.main_input_name]\n",
    "\n",
    "        tags = inputs[\"decoder_tags\"]\n",
    "        gen_kwargs.update({\"decoder_tags\": tags})\n",
    "        # print(f\"Gen kwargs: {gen_kwargs}\")\n",
    "        # print(f\"Gen inputs:{generation_inputs}\")\n",
    "        generated_tokens = self.model.generate(\n",
    "            generation_inputs,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if gen_kwargs.get(\"max_length\") is not None and generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "        elif gen_kwargs.get(\"max_new_tokens\") is not None and generated_tokens.shape[-1] < (\n",
    "            gen_kwargs[\"max_new_tokens\"] + 1\n",
    "        ):\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_new_tokens\"] + 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                with self.compute_loss_context_manager():\n",
    "                    outputs = model(**inputs) # linear_logits as output\n",
    "                if self.label_smoother is not None:\n",
    "                    loss = self.label_smoother(outputs, inputs[\"decoder_tags\"]).mean().detach()\n",
    "                else:\n",
    "                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "\n",
    "        if self.args.prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        if has_labels:\n",
    "            labels = inputs[\"decoder_tags\"]\n",
    "            if gen_kwargs.get(\"max_length\") is not None and labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "                labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
    "            elif gen_kwargs.get(\"max_new_tokens\") is not None and labels.shape[-1] < (\n",
    "                gen_kwargs[\"max_new_tokens\"] + 1\n",
    "            ):\n",
    "                labels = self._pad_tensors_to_max_len(labels, (gen_kwargs[\"max_new_tokens\"] + 1))\n",
    "        else:\n",
    "            labels = None\n",
    "        # print(labels)\n",
    "\n",
    "        return (loss, generated_tokens, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4e823b-e3d8-4b68-8a49-d327b130d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=4,\n",
    "    num_train_epochs=15,\n",
    "    predict_with_generate=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_rouge1\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=42,\n",
    "    generation_max_length=max_target_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12bd51c9-2099-497d-8c34-945230f12dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0fd3626-cfd1-4b71-b11c-64792462b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # print(\"In compute metrics\")\n",
    "    # print(predictions[0])\n",
    "    # print(labels[0])\n",
    "    \n",
    "    preds = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    print(result.items())\n",
    "    #result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab5cfb3f-6f29-40de-9f69-f421aac7e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MySeq2SeqTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df0508f1-b851-4c58-b532-df53b04932ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 06:11, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.579500</td>\n",
       "      <td>6.072535</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.889600</td>\n",
       "      <td>4.142837</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>78.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.691200</td>\n",
       "      <td>3.524900</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>32.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.862200</td>\n",
       "      <td>3.159065</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.396700</td>\n",
       "      <td>2.873141</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>25.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.597400</td>\n",
       "      <td>2.680887</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>25.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.062100</td>\n",
       "      <td>2.443796</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.828700</td>\n",
       "      <td>2.193723</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>22.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.528800</td>\n",
       "      <td>2.030260</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>22.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.286500</td>\n",
       "      <td>1.881976</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.967500</td>\n",
       "      <td>1.781471</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>21.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.573200</td>\n",
       "      <td>1.724208</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.508500</td>\n",
       "      <td>1.624778</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>22.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.320400</td>\n",
       "      <td>1.594604</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>22.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.522100</td>\n",
       "      <td>1.573230</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>22.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('rouge1', 0.26666666666666666), ('rouge2', 0.0), ('rougeL', 0.26666666666666666), ('rougeLsum', 0.26666666666666666)])\n",
      "dict_items([('rouge1', 0.03308361204013377), ('rouge2', 0.009523809523809523), ('rougeL', 0.03308361204013377), ('rougeLsum', 0.03308361204013378)])\n",
      "dict_items([('rouge1', 0.053333333333333344), ('rouge2', 0.015384615384615385), ('rougeL', 0.05345238095238096), ('rougeLsum', 0.05345238095238096)])\n",
      "dict_items([('rouge1', 0.05714285714285714), ('rouge2', 0.01666666666666667), ('rougeL', 0.05714285714285715), ('rougeLsum', 0.05714285714285715)])\n",
      "dict_items([('rouge1', 0.06062271062271063), ('rouge2', 0.01666666666666667), ('rougeL', 0.06062271062271063), ('rougeLsum', 0.06062271062271063)])\n",
      "dict_items([('rouge1', 0.07076923076923076), ('rouge2', 0.01818181818181818), ('rougeL', 0.07076923076923076), ('rougeLsum', 0.07076923076923076)])\n",
      "dict_items([('rouge1', 0.07174825174825175), ('rouge2', 0.02222222222222222), ('rougeL', 0.07174825174825175), ('rougeLsum', 0.07174825174825175)])\n",
      "dict_items([('rouge1', 0.06282051282051282), ('rouge2', 0.01818181818181818), ('rougeL', 0.06282051282051282), ('rougeLsum', 0.06282051282051282)])\n",
      "dict_items([('rouge1', 0.06190476190476191), ('rouge2', 0.01666666666666667), ('rougeL', 0.06190476190476191), ('rougeLsum', 0.06190476190476191)])\n",
      "dict_items([('rouge1', 0.06743589743589744), ('rouge2', 0.01818181818181818), ('rougeL', 0.06743589743589744), ('rougeLsum', 0.06743589743589744)])\n",
      "dict_items([('rouge1', 0.06410256410256411), ('rouge2', 0.01818181818181818), ('rougeL', 0.06410256410256411), ('rougeLsum', 0.06410256410256411)])\n",
      "dict_items([('rouge1', 0.06743589743589744), ('rouge2', 0.01818181818181818), ('rougeL', 0.06743589743589744), ('rougeLsum', 0.06743589743589744)])\n",
      "dict_items([('rouge1', 0.06743589743589744), ('rouge2', 0.01818181818181818), ('rougeL', 0.06743589743589744), ('rougeLsum', 0.06743589743589744)])\n",
      "dict_items([('rouge1', 0.06743589743589744), ('rouge2', 0.01818181818181818), ('rougeL', 0.06743589743589744), ('rougeLsum', 0.06743589743589744)])\n",
      "dict_items([('rouge1', 0.06743589743589744), ('rouge2', 0.01818181818181818), ('rougeL', 0.06743589743589744), ('rougeLsum', 0.06743589743589744)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=3.891774559020996, metrics={'train_runtime': 372.0294, 'train_samples_per_second': 3.226, 'train_steps_per_second': 0.806, 'total_flos': 650142716313600.0, 'train_loss': 3.891774559020996, 'epoch': 15.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31adef8e-cf9b-4cf8-a8e7-ffe310b72912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 23 15:21:00 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:A1:00.0 Off |                    0 |\n",
      "| N/A   57C    P0             285W / 300W |  76340MiB / 81920MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5684      C   .../miniconda3/envs/nlp_env/bin/python    67658MiB |\n",
      "|    0   N/A  N/A     26521      C   .../miniconda3/envs/nlp_env/bin/python     8664MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7396d-cb6f-4ea6-b2e4-de7ac3326504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
