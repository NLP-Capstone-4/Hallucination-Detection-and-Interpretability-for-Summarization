{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7378354-8f93-4ebe-9e9c-689d35f1f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmazaher/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4614969d-8568-40ed-ab8a-a05b987cf12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM = 500\n",
    "\n",
    "SEP_TOKEN = \" <SEP> \"\n",
    "EOS_TOKEN = \" <EOS>\"\n",
    "punctuations = ['!',',','.','?',\":\",\";\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e010dc-ae59-401f-8041-168b1d59bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_text(text):\n",
    "    curated_text = ''\n",
    "    if type(text) == str:\n",
    "        text = text.replace('\\n',' ')\n",
    "        for punctuation in punctuations:\n",
    "            text = text.replace(punctuation, ' ' + punctuation + ' ')\n",
    "        \n",
    "        curated_text = \" \".join(text.strip().split())\n",
    "    return curated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e013317-9d3d-4ce5-b69a-1b2b6fde3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source(row):\n",
    "    source = row['Dialogue']\n",
    "    return source.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfe290d-3fdf-4434-8e22-469c60e4cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tgt_sum(row):\n",
    "    tgt_sum = row['Generated Summary']\n",
    "    return tgt_sum.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dafb472-b185-4f9d-a7b1-e399f5800686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags(row):    \n",
    "    return row['Annotations'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f8c977-2480-4e51-ae8b-01d29782d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(tokens):\n",
    "    return len(tokens)\n",
    "\n",
    "def map_tag_ids(tags):\n",
    "    return tagLabels.str2int(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a20794-6d00-44a7-a30f-0ab2189a5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stratified_into_train_val_test(df_input, frac_train=0.8, frac_val=0.1, frac_test=0.1):\n",
    "    if frac_train + frac_val + frac_test != 1.0:\n",
    "        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n",
    "                         (frac_train, frac_val, frac_test))\n",
    "\n",
    "    # Split original dataframe into train and temp dataframes.\n",
    "    df_train, df_temp = train_test_split(df_input, test_size=(1.0 - frac_train), random_state=42, shuffle=True)\n",
    "\n",
    "    \n",
    "    # Split the temp dataframe into val and test dataframes.\n",
    "    relative_frac_test = frac_test / (frac_val + frac_test)\n",
    "    df_val, df_test = train_test_split(df_temp, test_size = relative_frac_test, random_state=42, shuffle=True)\n",
    "    \n",
    "    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)\n",
    "    \n",
    "    df_train.reset_index(drop=True, inplace = True)\n",
    "    df_val.reset_index(drop=True, inplace = True)\n",
    "    df_test.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f7d56f-e592-4fb8-9082-cbb945971350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('annotated_capstone_data.csv')\n",
    "\n",
    "df = df.iloc[FROM:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "720e4cbd-fbcc-48f4-be19-49845985a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dialogue'] = df['Dialogue'].apply(curate_text)\n",
    "df['Generated Summary'] = df['Generated Summary'].apply(curate_text)\n",
    "df['Annotations'] = df['Annotations'].apply(curate_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7169bb-d902-4af4-a5f1-b570f8c340b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = df.apply(create_source, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7dee49e-8497-45a7-8746-fdf81266f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_target'] = df.apply(create_tgt_sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829dcee4-39ea-41ae-8451-c34fd244ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df.apply(create_tags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bddfc0a-d751-4448-bd09-c192a7681273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_labels = np.unique(df['tags'].sum()).tolist()\n",
    "tagLabels = datasets.ClassLabel(num_classes=len(tag_labels), names=tag_labels)\n",
    "\n",
    "df['tag_ids'] = df['tags'].apply(map_tag_ids)\n",
    "df['gold_tags'] = df['summary_target'].apply(lambda x: [6] * len(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d0bf720-4e55-422f-87af-d21f267ef268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'summary_target': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'tags': Sequence(feature=ClassLabel(names=['C', 'M', 'N', 'O', 'OB', 'W'], id=None), length=-1, id=None), 'gold_tags': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "ds_features = datasets.Features({\n",
    "    'source': datasets.Sequence(feature=datasets.Value(dtype='string', id=None), length=-1, id=None), \n",
    "    'summary_target': datasets.Sequence(feature=datasets.Value(dtype='string', id=None), length=-1, id=None),\n",
    "    'tags': datasets.Sequence(feature=tagLabels, length=-1, id=None),\n",
    "    'gold_tags': datasets.Sequence(feature=datasets.Value(dtype='int32', id=None), length=-1, id=None),\n",
    "})\n",
    "print(ds_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c468ec2-89b3-48c9-ace0-23d09d220040",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame()\n",
    "dataset_df['source'] = df['source']\n",
    "dataset_df['summary_target'] = df['summary_target']\n",
    "dataset_df['tags'] = df['tag_ids']\n",
    "dataset_df['gold_tags'] = df['gold_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dc3a72b-7417-420a-8da3-c62efde33d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_df['summary_target'][500]) == len(dataset_df['tags'][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e947acca-6967-4963-a5be-d6149d633b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = split_stratified_into_train_val_test(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba889613-5ad4-4dcb-962f-dad7ff90de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.Dataset.from_pandas(df_train, features=ds_features, split='train')\n",
    "val_ds = datasets.Dataset.from_pandas(df_val, features=ds_features, split='validation')\n",
    "test_ds = datasets.Dataset.from_pandas(df_test, features=ds_features, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19bb3ac9-0c82-46c7-8288-74487b742c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 527.85ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 707.66ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Downloading metadata: 100%|██████████| 685/685 [00:00<00:00, 3.70MB/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 723.28ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Downloading metadata: 100%|██████████| 798/798 [00:00<00:00, 4.68MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token = \"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\"\n",
    "dataset_name = 'samsum'\n",
    "train_ds.push_to_hub(f'pvisnrt/{dataset_name}', token=token)\n",
    "val_ds.push_to_hub(f'pvisnrt/{dataset_name}', token=token)\n",
    "test_ds.push_to_hub(f'pvisnrt/{dataset_name}', token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c7913-54c5-4271-a82f-bdb0f5f0a776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samsum adding Ground Truth (O) column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "samsum_dataset = load_dataset(\"samsum\")\n",
    "\n",
    "samsum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to tokenize summary and create 'O' sequence\n",
    "def create_o_sequence(summary):\n",
    "    tokens = summary.split()  # Simple tokenization by splitting on whitespace\n",
    "    o_sequence = [\"O\"] * len(tokens)\n",
    "    return ' '.join(o_sequence)\n",
    "\n",
    "# Iterate through each split and modify the dataset\n",
    "for split in samsum_dataset.keys():\n",
    "    # Add a new column with 'O' sequence for each summary\n",
    "    samsum_dataset[split] = samsum_dataset[split].map(lambda x: {\"gold_tags\": create_o_sequence(x[\"summary\"])})\n",
    "\n",
    "\n",
    "\n",
    "#  push it to the hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 15/15 [00:00<00:00, 305.50ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 320.74ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 400.22ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "token = \"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\"\n",
    "samsum_dataset.push_to_hub(\"pvisnrt/samsum\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13818513',\n",
       " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
       " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
       " 'gold_tags': 'O O O O O O O O O'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum_dataset['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
