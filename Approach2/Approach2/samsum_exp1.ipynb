{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfb3f14-7161-4734-a832-3a0762d481d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re, json\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_metric,Dataset,DatasetDict, load_dataset, Sequence, Value\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, BartForConditionalGeneration\n",
    "from transformers import AutoTokenizer, Trainer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "from jointbart_step2 import myBartForConditionalGeneration\n",
    "from hg_utils import GenerationMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c05fd6-23ac-4055-845e-fd2c4fdb835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce90aa0-7033-427e-bc29-ee148a1acc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65402bdf-8983-45b7-b22a-8a576edf5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9af4c39-926f-407f-893b-346339dba231",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"hallucination-tagging-classifier\"\n",
    "metric = evaluate.load(\"rouge\")\n",
    "model = myBartForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\", add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03ebaed-5107-4492-8d2a-9082efa6b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name == 'classifier.weight' or name == 'classifier.bias':\n",
    "        param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4251206d-463c-42a1-827e-32cb25c25ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('pvisnrt/special_samsum')\n",
    "id2label =  {0: 'C', 1: 'M', 2: 'N', 3: 'O', 4: 'OB', 5: 'W'}\n",
    "label2id = {'C': 0, 'M': 1, 'N': 2, 'O': 3, 'OB': 4, 'W': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b56648b-703d-4b9f-bdff-128e6df29147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'tags', 'tag_ids'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'tags', 'tag_ids'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'tags', 'tag_ids'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c76e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['train'] = dataset['train'].cast_column(\"tag_ids\", Sequence(Value(\"int32\")))\n",
    "# dataset['validation'] = dataset['validation'].cast_column(\"tag_ids\", Sequence(Value(\"int32\")))\n",
    "# dataset['test'] = dataset['test'].cast_column(\"tag\", Sequence(Value(\"int32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a31e95-f76f-428d-b5e1-2cc00e7b6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    inputs = [doc for doc in examples['dialogue']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, is_split_into_words=True, return_tensors='pt', padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_inputs = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True, is_split_into_words=True, return_tensors='pt', padding='max_length')\n",
    "\n",
    "    labels = []\n",
    "    summary_len = []\n",
    "    tags_len = []\n",
    "    tags_ids_len = []\n",
    "    for i, label in enumerate(examples[\"tag_ids\"]):\n",
    "        summary_len.append(len(examples['summary'][i]))\n",
    "        tags_len.append(len(examples['tags'][i]))\n",
    "        tags_ids_len.append(len(examples['tag_ids'][i]))\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)# Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            \n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    model_inputs['labels'] = tokenized_inputs['input_ids']\n",
    "\n",
    "    for i, t in zip(model_inputs['labels'], labels):\n",
    "        if len(i) != len(t):\n",
    "            print(\"Issue\")\n",
    "\n",
    "    model_inputs[\"decoder_tags\"] = labels\n",
    "    \n",
    "    model_inputs['summary_len'] = summary_len\n",
    "    model_inputs['tags_len'] = tags_len\n",
    "    model_inputs['tags_ids_len'] = tags_ids_len\n",
    "     \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95cf1276-e3fe-4cb8-9909-c1f813346061",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc971df8-5991-4ecc-a970-fb19ffedd4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'tags', 'tag_ids', 'input_ids', 'attention_mask', 'labels', 'decoder_tags', 'summary_len', 'tags_len', 'tags_ids_len'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'tags', 'tag_ids', 'input_ids', 'attention_mask', 'labels', 'decoder_tags', 'summary_len', 'tags_len', 'tags_ids_len'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'tags', 'tag_ids', 'input_ids', 'attention_mask', 'labels', 'decoder_tags', 'summary_len', 'tags_len', 'tags_ids_len'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2af46311-0248-4620-bab0-c46d17bb4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(['id', 'dialogue', 'summary', 'tags', 'tag_ids'])\n",
    "tokenized_datasets['validation'] = tokenized_datasets['validation'].remove_columns(['id', 'dialogue', 'summary', 'tags', 'tag_ids'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(['id', 'dialogue', 'summary', 'tags', 'tag_ids'])\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(['tags_ids_len', 'summary_len', 'tags_len'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(['tags_ids_len', 'summary_len', 'tags_len'])\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].select(range(100))\n",
    "tokenized_datasets['validation'] = tokenized_datasets['validation'].select(range(20))\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].select(range(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d49d5f-d68d-4111-ac74-e97f9e2c4a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels', 'decoder_tags'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c9cecd-5bed-412a-9f60-6afd16f443d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "        Return:\n",
    "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n",
    "            labels (each being optional).\n",
    "        \"\"\"\n",
    "        summary_len = inputs['summary_len'].cpu().detach().tolist()\n",
    "        tag_ids_len = inputs['tags_ids_len'].cpu().detach().tolist()\n",
    "        tags_len = inputs['tags_len'].cpu().detach().tolist()\n",
    "        inputs.pop('tags_ids_len')\n",
    "        inputs.pop('summary_len')\n",
    "        inputs.pop('tags_len')\n",
    "        if not self.args.predict_with_generate or prediction_loss_only:\n",
    "            return super().prediction_step(\n",
    "                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n",
    "            )\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        # print(\"prediction_step inputs: {}\".format(inputs.keys()))\n",
    "\n",
    "        # XXX: adapt synced_gpus for fairscale as well\n",
    "        gen_kwargs = self._gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
    "            gen_kwargs[\"max_length\"] = self.model.config.max_length\n",
    "        gen_kwargs[\"num_beams\"] = (\n",
    "            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.model.config.num_beams\n",
    "        )\n",
    "        # default_synced_gpus = True if is_deepspeed_zero3_enabled() else False\n",
    "        default_synced_gpus = False\n",
    "        gen_kwargs[\"synced_gpus\"] = (\n",
    "            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n",
    "        )\n",
    "\n",
    "        if \"attention_mask\" in inputs:\n",
    "            gen_kwargs[\"attention_mask\"] = inputs.get(\"attention_mask\", None)\n",
    "        if \"global_attention_mask\" in inputs:\n",
    "            gen_kwargs[\"global_attention_mask\"] = inputs.get(\"global_attention_mask\", None)\n",
    "\n",
    "        # prepare generation inputs\n",
    "        # some encoder-decoder models can have varying encoder's and thus\n",
    "        # varying model input names\n",
    "        if hasattr(self.model, \"encoder\") and self.model.encoder.main_input_name != self.model.main_input_name:\n",
    "            generation_inputs = inputs[self.model.encoder.main_input_name]\n",
    "        else:\n",
    "            generation_inputs = inputs[self.model.main_input_name]\n",
    "\n",
    "        tags = inputs[\"decoder_tags\"]\n",
    "        gen_kwargs.update({\"decoder_tags\": tags})\n",
    "        # print(f\"Gen kwargs: {gen_kwargs}\")\n",
    "        # print(f\"Gen inputs:{generation_inputs}\")\n",
    "         #generated_tokens = self.model.generate(\n",
    "        #    generation_inputs,\n",
    "        #    **gen_kwargs,\n",
    "        #)\n",
    "        \n",
    "        gen_mix = GenerationMixin(model)\n",
    "        generated_tokens, classification_ids = gen_mix.generate(generation_inputs, **gen_kwargs)\n",
    "        \n",
    "        dialog = tokenizer.batch_decode(generation_inputs, skip_special_tokens=True)\n",
    "        print('-'*89)\n",
    "        print('dialog: ', dialog)\n",
    "        \n",
    "        generated_summaries = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        print('Generated Summaries:\\n',*generated_summaries, sep='\\n')\n",
    "        print(f'Generated summary length: {generated_tokens.shape}')\n",
    "        print(f\"Gold summary length: {summary_len}\")\n",
    "        \n",
    "        classification_labels = []\n",
    "        classification_ids_lst = classification_ids.cpu().detach().tolist()\n",
    "        for batch_classification_ids in classification_ids_lst:\n",
    "            batch_classification_labels = []\n",
    "            for classification_id in batch_classification_ids:\n",
    "                classification_id = classification_id - 3\n",
    "                if classification_id >= 0 and classification_id < len(id2label):\n",
    "                    batch_classification_labels.append(id2label[classification_id])\n",
    "            \n",
    "            classification_labels.append(' '.join(batch_classification_labels))\n",
    "                \n",
    "        print('Generated Classification Labels:\\n',*classification_labels, sep='\\n')\n",
    "        print(f'Generated classification tag length: {classification_ids.shape}')\n",
    "        print(f\"Gold Classification tag length: {tags_len}\")\n",
    "        print(f\"Gold Classification tag ids length: {tag_ids_len}\")\n",
    "    \n",
    "       \n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if gen_kwargs.get(\"max_length\") is not None and generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "        elif gen_kwargs.get(\"max_new_tokens\") is not None and generated_tokens.shape[-1] < (\n",
    "            gen_kwargs[\"max_new_tokens\"] + 1\n",
    "        ):\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_new_tokens\"] + 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                with self.compute_loss_context_manager():\n",
    "                    outputs = model(**inputs) # lm_logits as output\n",
    "                if self.label_smoother is not None:\n",
    "                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
    "                else:\n",
    "                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "\n",
    "        if self.args.prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        if has_labels:\n",
    "            labels = inputs[\"labels\"]\n",
    "            if gen_kwargs.get(\"max_length\") is not None and labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "                labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
    "            elif gen_kwargs.get(\"max_new_tokens\") is not None and labels.shape[-1] < (\n",
    "                gen_kwargs[\"max_new_tokens\"] + 1\n",
    "            ):\n",
    "                labels = self._pad_tensors_to_max_len(labels, (gen_kwargs[\"max_new_tokens\"] + 1))\n",
    "        else:\n",
    "            labels = None\n",
    "        # print(labels)\n",
    "\n",
    "        return (loss, generated_tokens, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d87ebd78-55ab-41a5-8086-cb2865a49e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"checkpoints/\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=4,\n",
    "#     num_train_epochs=10,\n",
    "#     predict_with_generate=True,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     fp16=True,\n",
    "#     logging_steps=1,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     greater_is_better=True,\n",
    "#     metric_for_best_model='Rouge1',\n",
    "#     load_best_model_at_end=True,\n",
    "#     seed=42,\n",
    "#     generation_max_length=max_target_length,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20cb54-5565-4e9e-9bfe-f01169120cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cfb2c32-e9c3-4808-a9a8-1db975df8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints/\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    seed=42,\n",
    "    generation_max_length=max_target_length,\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns =False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "076293cd-d640-4f71-ac7c-6b5ec2147236",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7231598e-f79c-431d-9e52-e0e437f543e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Generated summary: {decoded_preds[0]}\")\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Gold summary: {decoded_labels[0]}\")\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b8a2f89-e2fc-4efa-9878-3280d7f31f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MySeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e1dbddf-e03f-4669-87a7-9d0557b74176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdevavratj\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Q4\\capstone\\approach2\\New Approach\\wandb\\run-20231128_023130-y5hswzl3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devavratj/huggingface/runs/y5hswzl3' target=\"_blank\">distinctive-deluge-60</a></strong> to <a href='https://wandb.ai/devavratj/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devavratj/huggingface' target=\"_blank\">https://wandb.ai/devavratj/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devavratj/huggingface/runs/y5hswzl3' target=\"_blank\">https://wandb.ai/devavratj/huggingface/runs/y5hswzl3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 05:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.332300</td>\n",
       "      <td>4.466559</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.345300</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>23.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.455700</td>\n",
       "      <td>3.383375</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>27.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" A : Hi Tom, are you busy tomorrow’s afternoon? B : I’m pretty sure I am. What’s up? A : Can you go with me to the animal shelter?. B : What do you want to do? A : I want to get a puppy for my son. B : That will make him so happy. A : Yeah, we’ve discussed it many times. I think he’s ready now. B : That’s good. Raising a dog is a tough issue. Like having a baby ; -) A : I'll get him one of those little dogs. B : One that won't grow up too big ; -) A : And eat too much ; -)) B : Do you know which one he would like? A : Oh, yes, I took him there last Monday. He showed me one that he really liked. B : I bet you had to drag him away. A : He wanted to take it home right away ; -). B : I wonder what he'll name it. A : He said he’d name it after his dead hamster – Lemmy - he's a great Motorhead fan : -)))\"]\n",
      "Generated Summaries:\n",
      "\n",
      " A wants to get a puppy for her son. She'll take him to the animal shelter to get him one of those little dogs.\n",
      "Generated summary length: torch.Size([1, 30])\n",
      "Gold summary length: [30]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 29])\n",
      "Gold Classification tag length: [30]\n",
      "Gold Classification tag ids length: [30]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Emma : I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob : I used to get one every year as a child! Loved them! Emma : Yeah, i remember! they were filled with chocolates! Lauren : they are different these days! much more sophisticated! Haha! Rob : yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma : what do you fit inside? Lauren : small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma : WOW! That’s brill! X Lauren : i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob : i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren : i reckon it prepares them for Christmas Emma : and makes it more about traditions and being kind to other people Lauren : my children get very excited every time they get one! Emma : i can see why! : )']\n",
      "Generated Summaries:\n",
      "\n",
      " Emma has just fallen in love with the advent calendar. She wants to get one for her kids for Christmas.\n",
      "Generated summary length: torch.Size([1, 25])\n",
      "Gold summary length: [34]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 42])\n",
      "Gold Classification tag length: [34]\n",
      "Gold Classification tag ids length: [34]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Jackie : Madison is pregnant Jackie : but she doesn't wanna talk about it Iggy : why Jackie : I don't know why because she doesn't wanna talk about it Iggy : ok Jackie : I wanted to prepare you for it because people get super excited and ask lots of questions Jackie : and she looked way more anxious than excited Iggy : she's probably worrying about it Iggy : she's taking every commitment really seriously Jackie : it could be money problems or relationship problems Iggy : or maybe she wants an abortion Jackie : it could be all of the above Iggy : but you know what? Iggy : once my friend was pregnant and I couldn't bring myself to be happy about it Jackie : why? Iggy : I felt they were immature and I couldn't picture this couple as parents Jackie : I felt similar way on Patricia's wedding Iggy : Patricia Stevens? Jackie : yes Iggy : so we're talking about the same person Jackie : what a coincidence Jackie : so she's pregnant? Iggy : she thought she was Jackie : damn...\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Jackie is pregnant and she doesn't want to talk about it. Iggy is worried about it because she's taking every commitment really seriously.\n",
      "Generated summary length: torch.Size([1, 31])\n",
      "Gold summary length: [23]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 30])\n",
      "Gold Classification tag length: [23]\n",
      "Gold Classification tag ids length: [23]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Marla : <file_photo> Marla : look what I found under my bed Kiki : lol Tamara : is that someone's underwear? Marla : it certainly isn't mine, my ass is big but it isn't huge Kiki : it looks like male underwear Tamara : not necessarily, maybe some butch had fun in your room while you were gone Marla : ok but how can you leave your underwear after hooking up? wtf is wrong with people Kiki : she or he could be too wasted to notice Tamara : or maybe someone put their pants there to piss you off Marla : that makes no sense Marla : it's so fucking childish Kiki : if it's childish then it must have been your sister's idea Marla : she's 13, she doesn't have underwear that isn't pink Tamara : maybe it belonged to one of your exes? Kiki : she would have recognized it Marla : lol we're doing total CSI investigation on one pair of boxers : D Kiki : <file_gif> Tamara : lol Tamara : I think your sister convinced someone to put their underwear in your room as a dare Marla : sounds legit Kiki : Tamara, you just\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Marla found a pair of male underwear under her bed. She thinks it was someone else's underwear.\n",
      "Generated summary length: torch.Size([1, 24])\n",
      "Gold summary length: [10]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 33])\n",
      "Gold Classification tag length: [10]\n",
      "Gold Classification tag ids length: [10]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Robert : Hey give me the address of this music shop you mentioned before Robert : I have to buy guitar cable Fred : <file_other> Fred : Catch it on google maps Robert : thx m8 Fred : ur welcome']\n",
      "Generated Summaries:\n",
      "\n",
      " Fred has to buy a guitar cable from the music shop Robert mentioned before.\n",
      "Generated summary length: torch.Size([1, 18])\n",
      "Gold summary length: [20]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 17])\n",
      "Gold Classification tag length: [20]\n",
      "Gold Classification tag ids length: [20]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Keith : Meg, pls buy some milk and cereals, I see now we've run out of them Megan : hm, sure, I can do that Megan : but did you check in the drawer next to the fridge? Keith : nope, let me have a look Keith : ok, false alarm, we have cereal and milk : D Megan : <file_gif>\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Megan will buy some milk and cereals.\n",
      "Generated summary length: torch.Size([1, 12])\n",
      "Gold summary length: [16]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 18])\n",
      "Gold Classification tag length: [16]\n",
      "Gold Classification tag ids length: [16]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Samantha : <file_video> Evelyn : LOL Holly : Is SHE making that noise?? Samatha : Yes (＾▽＾) Holly : How possible?? : o Samantha : Idk, I'm also surprised!! Evelyn : xD\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Samatha is making a noise. Holly is surprised.\n",
      "Generated summary length: torch.Size([1, 14])\n",
      "Gold summary length: [17]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 18])\n",
      "Gold Classification tag length: [17]\n",
      "Gold Classification tag ids length: [17]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Theresa : have you been at Tom's new place? Luis : yes, it's nice Marion : He invited us for a dinner Adam : where is it? Marion : a bit outside the city Adam : where exactly? Marion : Fiesole Luis : very nice!\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Theresa and Luis are going to Tom's new place for a dinner.\n",
      "Generated summary length: torch.Size([1, 17])\n",
      "Gold summary length: [14]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 17])\n",
      "Gold Classification tag length: [14]\n",
      "Gold Classification tag ids length: [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Jane : Hello Vegano Resto : Hello, how may I help you today? Jane : I would like to make a reservation. Jane : For 6 people, tonight around 20 : 00 Vegano Resto : Let me just check. Vegano Resto : Ah, I'm afraid that there is no room at 20 : 00. Vegano Resto : However, I could offer you a table for six at 18 : 30 or at 21 : 00 Vegano Resto : Would either of those times suit you? Jane : Oh dear. Jane : Let me just ask my friends. Vegano Resto : No problem. Jane : 21 : 00 will be ok. Vegano Resto : Perfect. So tonight at 21 : 00 for six people under your name. Jane : great, thank you!\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Jane would like to make a reservation for six people at 21 : 00. Vegano Resto will make the reservation for Jane at 18 : 30.\n",
      "Generated summary length: torch.Size([1, 33])\n",
      "Gold summary length: [14]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 44])\n",
      "Gold Classification tag length: [14]\n",
      "Gold Classification tag ids length: [14]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Nancy : Howdy, how y\\'all doin\\'? Tina : Is that a Texan drawl, girl? Nancy : Yes ma\\'am! Loving it out here! Tina : How\\'s the job going? Kids behaving themselves? Nancy : Mostly! They laugh at my accent though! Tina : Well, they probably haven\\'t met a Welsh person before! Nancy : No shit! They ask me to repeat everything! Best one is \"Water\", course, it\\'s mostly \"Waarderr\" here! Tina : LOL. I\\'d love to hear that, you picked up the accent yet? Nancy : Nah, 21 years in Cardiff isn\\'t easily removed! Tina : We\\'re missing you here, the pub is quiet these days without your laugh! Nancy : Miss you too! I\\'m coming home in 6 weeks, though. Last fortnight I\\'m going travelling with 3 other Brits working here, a Geordie girl, a guy from Belfast and Annie, who\\'s from Glasgow. Tina : My God, I\\'m so jealous! I bet they had even more trouble being understood out there! See you after your trip!']\n",
      "Generated Summaries:\n",
      "\n",
      " Nancy is working at a pub in Cardiff, and she is having trouble being understood because she has a Texan accent. She is going travelling to Belfast in 6 weeks.\n",
      "Generated summary length: torch.Size([1, 37])\n",
      "Gold summary length: [33]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 45])\n",
      "Gold Classification tag length: [33]\n",
      "Gold Classification tag ids length: [33]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Laura : I need a new printer : / Laura : thinking about this one Laura : <file_other> Jamie : you're sure you need a new one? Jamie : I mean you can buy a second hand one Laura : could be\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Laura needs a new printer. Jamie is thinking about buying a second hand one.\n",
      "Generated summary length: torch.Size([1, 19])\n",
      "Gold summary length: [8]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 18])\n",
      "Gold Classification tag length: [8]\n",
      "Gold Classification tag ids length: [8]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Barbara : got everything? Haylee : yeah almost Haylee : i'm in dairy section Haylee : but can't find this youghurt u wanted Barbara : the coconut milk one? Haylee : yeah Barbara : hmmm yeah that's a mystery. cause it's not dairy but it's yoghurt xD Haylee : exactly xD Haylee : ok i asked sb. they put it next to eggs lol Barbara : lol\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Haylee is in the dairy section but she can't find the coconut milk youghurt she wanted.\n",
      "Generated summary length: torch.Size([1, 24])\n",
      "Gold summary length: [8]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 29])\n",
      "Gold Classification tag length: [8]\n",
      "Gold Classification tag ids length: [8]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Norbert : we need to hurry to catch the tour Wendy : ok, am buying something. be right out! Norbert : ok. am not waiting long though. missed the last one because of you Wendy : just be patient for once. Norbert : im always patient Wendy : at the register now Norbert : alright']\n",
      "Generated Summaries:\n",
      "\n",
      " Norbert will buy something at the cash register.\n",
      "Generated summary length: torch.Size([1, 13])\n",
      "Gold summary length: [15]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 22])\n",
      "Gold Classification tag length: [15]\n",
      "Gold Classification tag ids length: [15]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Lidia : hi guys, how was your day? Cecil : amazing Lidia : where did you go? Cheryl : to the Jandia Peninsula Cheryl : sorry, Cecil is driving Lidia : and how was it? Cheryl : I liked it a lot Cheryl : Peter took very nice pics Peter : <file_photo> <file_photo> Peter : but it was very windy Lidia : yes, it's always windy here Peter : really? Also in summer? Lidia : sure, the name Fuerteventura means strong wind Cheryl : wow, it's fascinating Lidia : so do you have any plans for tomorrow Cheryl : Cecil wants to explore more the south of the island Peter : I'm just a passenger, so have no voice Cheryl : c'mon, it's not true Peter : I'm just joking Cheryl : we will decide after dinner Cecil : ok, so let me know Cheryl : we will\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Lidia and Cecil are going to Fuerteventura tomorrow. They will decide after dinner. Cheryl will take Cecil to the Jandia Peninsula.\n",
      "Generated summary length: torch.Size([1, 34])\n",
      "Gold summary length: [34]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 36])\n",
      "Gold Classification tag length: [34]\n",
      "Gold Classification tag ids length: [34]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Nickola : Have you found it? Sophie : No! Still looking : ( Nickola : Check pockets and handbags. Sophie : Checked them all twice already...']\n",
      "Generated Summaries:\n",
      "\n",
      " Sophie is still looking for Nickola's purse.\n",
      "Generated summary length: torch.Size([1, 13])\n",
      "Gold summary length: [12]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 13])\n",
      "Gold Classification tag length: [12]\n",
      "Gold Classification tag ids length: [12]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Rosie : What\\'s your favorite b-movie? Elle : um, hard to say. Why do you ask? Dennis : Toxic avenger for sure Rosie : I have to write an essay and I chose bad movies as my topic and I\\'m just looking for inspiration Elle : plan 9 from outer space is definitely something worth mentioning Rosie : Yeah, I\\'ve seen it. And I will also cover \"The Room\". I\\'m just looking for something a bit more niche Dennis : There\\'s Troma Studio for ya - toxic avenger, poultrygeist - the latter is exceptionally awful - and it\\'s a musical Rosie : Is it one of those intentionally bad movies? Dennis : most definitely Rosie : ok, thank you, I\\'ll check it out Elle : oh, there\\'s also jesus christ vampire hunter Rosie : what? : D Elle : it\\'s even worse than it sounds Dennis : and when it comes to more recent movies there are those stupid animal-based horror movies like sharknado or zombeavers Rosie : I\\'ve heard of sharknado and zombeavers sound just awesome Rosie : thanks guys, you helped me a lot : )']\n",
      "Generated Summaries:\n",
      "\n",
      " Rosie has to write an essay based on a bad movie. She wants to cover \"The Room\" and \"The Toxic Avenger\". She will also cover \"Zombeavers\".\n",
      "Generated summary length: torch.Size([1, 41])\n",
      "Gold summary length: [14]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 40])\n",
      "Gold Classification tag length: [14]\n",
      "Gold Classification tag ids length: [14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Julia : What is your biggest dream Julia : I mean the kind that can be achieved James : Everyone say I have nice voice James : My mom liked very much when I was reading outloud James : I've had this dream for some time now, to become a voice actor James : Be a part of cartoon or video game as a voice actor reading a character Julia : Wow. Nice one. Julia : Btw you do have a nice voice Julia : I could listen to you as a radio speaker. James : Thanks James : I've worked in radio, but it was during college so I had little time for this Julia : Shame. James : I know. But nothing is lost. I still have microphone at home and with a bit of help I could make homemade radio station Julia : That's actually a great idea Julia : I cheer for you!\"]\n",
      "Generated Summaries:\n",
      "\n",
      " James has a good voice. He would like to become a voice actor. He has a microphone at home and could make a radio station.\n",
      "Generated summary length: torch.Size([1, 31])\n",
      "Gold summary length: [18]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 31])\n",
      "Gold Classification tag length: [18]\n",
      "Gold Classification tag ids length: [18]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Poppy : I literally cannot think any more today! Alice : Yeah, I'm in the same shape. What a long day! Poppy : Lunch went by in a flash because I had errands, which makes the day so slow! Alice : I didn't get lunch, so that's even worse! Poppy : Oh, poor you! Aren't you starving? Alice : I'll live. Only three more hours! Poppy : LOL! Not that you're counting... Alice : Damn straight I'm counting! LOL! Poppy : Well, I'm def going for drinks after work. Want to join? Alice : Who else is there? Poppy : Nobody yet but I was going to put the word out. Alice : Sure, sounds fun. I'll invite some people up here if that's okay? Poppy : Oh, got your eye on anyone? Alice : Fred! Poppy : Fred? Really? Alice : Sure, why not? He's single, my age and not bad looking. Poppy : He's a dork! Alice : But a cute one! Poppy : If you say so. Not my type! Alice : That's a relief! Poppy : He's all yours! Alice :\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Alice and Poppy are going for drinks after work.\n",
      "Generated summary length: torch.Size([1, 14])\n",
      "Gold summary length: [31]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 27])\n",
      "Gold Classification tag length: [31]\n",
      "Gold Classification tag ids length: [31]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Sash : need to see u Caron : y Caron : i'm out from 12 Sash : will be before Sash : then Caron : k Sash : open the door : Caron : what time u coming I need to go out Sash : soon Caron : hurry up I need to go out\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Sash is going to see Caron at 12. Caron will be out from 12.\n",
      "Generated summary length: torch.Size([1, 22])\n",
      "Gold summary length: [11]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 23])\n",
      "Gold Classification tag length: [11]\n",
      "Gold Classification tag ids length: [11]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Giuseppe : Hi man Matteo : Yo Giuseppe : How's it going with Gosia? Matteo : I don't know, she's a little strange Giuseppe : Why? Matteo : She always criticizes me because I like football and video games Giuseppe : Damn Matteo : Yeah... Giuseppe : Ok, I don't like games either, but... Matteo : You boring guy Giuseppe : Lol Matteo : Anyway I like her a lot Giuseppe : I can understand that, she's hot, if you ever dump her make sure you tell me Matteo : Get your hands off her, man Giuseppe : Just kidding Matteo : Lollolol\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Matteo is jealous of Gosia because she doesn't like football and video games.\n",
      "Generated summary length: torch.Size([1, 20])\n",
      "Gold summary length: [15]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 31])\n",
      "Gold Classification tag length: [15]\n",
      "Gold Classification tag ids length: [15]\n",
      "Generated summary:  A wants to get a puppy for her son. She'll take him to the animal shelter to get him one of those little dogs.\n",
      "Gold summary:  A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" A : Hi Tom, are you busy tomorrow’s afternoon? B : I’m pretty sure I am. What’s up? A : Can you go with me to the animal shelter?. B : What do you want to do? A : I want to get a puppy for my son. B : That will make him so happy. A : Yeah, we’ve discussed it many times. I think he’s ready now. B : That’s good. Raising a dog is a tough issue. Like having a baby ; -) A : I'll get him one of those little dogs. B : One that won't grow up too big ; -) A : And eat too much ; -)) B : Do you know which one he would like? A : Oh, yes, I took him there last Monday. He showed me one that he really liked. B : I bet you had to drag him away. A : He wanted to take it home right away ; -). B : I wonder what he'll name it. A : He said he’d name it after his dead hamster – Lemmy - he's a great Motorhead fan : -)))\"]\n",
      "Generated Summaries:\n",
      "\n",
      " A wants to take her son to the animal shelter to get a puppy for him. She wants to get him one of those little dogs. She took him to the shelter last Monday and he really liked one of the dogs.\n",
      "Generated summary length: torch.Size([1, 50])\n",
      "Gold summary length: [30]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 60])\n",
      "Gold Classification tag length: [30]\n",
      "Gold Classification tag ids length: [30]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Emma : I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids! Rob : I used to get one every year as a child! Loved them! Emma : Yeah, i remember! they were filled with chocolates! Lauren : they are different these days! much more sophisticated! Haha! Rob : yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff Emma : what do you fit inside? Lauren : small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets Emma : WOW! That’s brill! X Lauren : i add one more very special thing as well- little notes asking my children to do something nice for someone else Rob : i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc Lauren : i reckon it prepares them for Christmas Emma : and makes it more about traditions and being kind to other people Lauren : my children get very excited every time they get one! Emma : i can see why! : )']\n",
      "Generated Summaries:\n",
      "\n",
      " Emma has fallen in love with the advent calendar. She wants to get one for her kids for Christmas.\n",
      "Generated summary length: torch.Size([1, 24])\n",
      "Gold summary length: [34]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 41])\n",
      "Gold Classification tag length: [34]\n",
      "Gold Classification tag ids length: [34]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Jackie : Madison is pregnant Jackie : but she doesn't wanna talk about it Iggy : why Jackie : I don't know why because she doesn't wanna talk about it Iggy : ok Jackie : I wanted to prepare you for it because people get super excited and ask lots of questions Jackie : and she looked way more anxious than excited Iggy : she's probably worrying about it Iggy : she's taking every commitment really seriously Jackie : it could be money problems or relationship problems Iggy : or maybe she wants an abortion Jackie : it could be all of the above Iggy : but you know what? Iggy : once my friend was pregnant and I couldn't bring myself to be happy about it Jackie : why? Iggy : I felt they were immature and I couldn't picture this couple as parents Jackie : I felt similar way on Patricia's wedding Iggy : Patricia Stevens? Jackie : yes Iggy : so we're talking about the same person Jackie : what a coincidence Jackie : so she's pregnant? Iggy : she thought she was Jackie : damn...\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Madison is pregnant and she doesn't want to talk about it. Jackie wants to prepare Iggy for it because she wants to get an abortion.\n",
      "Generated summary length: torch.Size([1, 33])\n",
      "Gold summary length: [23]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 47])\n",
      "Gold Classification tag length: [23]\n",
      "Gold Classification tag ids length: [23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Marla : <file_photo> Marla : look what I found under my bed Kiki : lol Tamara : is that someone's underwear? Marla : it certainly isn't mine, my ass is big but it isn't huge Kiki : it looks like male underwear Tamara : not necessarily, maybe some butch had fun in your room while you were gone Marla : ok but how can you leave your underwear after hooking up? wtf is wrong with people Kiki : she or he could be too wasted to notice Tamara : or maybe someone put their pants there to piss you off Marla : that makes no sense Marla : it's so fucking childish Kiki : if it's childish then it must have been your sister's idea Marla : she's 13, she doesn't have underwear that isn't pink Tamara : maybe it belonged to one of your exes? Kiki : she would have recognized it Marla : lol we're doing total CSI investigation on one pair of boxers : D Kiki : <file_gif> Tamara : lol Tamara : I think your sister convinced someone to put their underwear in your room as a dare Marla : sounds legit Kiki : Tamara, you just\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Marla found a pair of male underwear under her bed. She thinks her sister put it there to piss her off.\n",
      "Generated summary length: torch.Size([1, 27])\n",
      "Gold summary length: [10]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 34])\n",
      "Gold Classification tag length: [10]\n",
      "Gold Classification tag ids length: [10]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Robert : Hey give me the address of this music shop you mentioned before Robert : I have to buy guitar cable Fred : <file_other> Fred : Catch it on google maps Robert : thx m8 Fred : ur welcome']\n",
      "Generated Summaries:\n",
      "\n",
      " Fred has to buy a guitar cable from the music shop Robert mentioned before.\n",
      "Generated summary length: torch.Size([1, 18])\n",
      "Gold summary length: [20]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 17])\n",
      "Gold Classification tag length: [20]\n",
      "Gold Classification tag ids length: [20]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Keith : Meg, pls buy some milk and cereals, I see now we've run out of them Megan : hm, sure, I can do that Megan : but did you check in the drawer next to the fridge? Keith : nope, let me have a look Keith : ok, false alarm, we have cereal and milk : D Megan : <file_gif>\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Megan and Keith have run out of milk and cereals.\n",
      "Generated summary length: torch.Size([1, 15])\n",
      "Gold summary length: [16]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 20])\n",
      "Gold Classification tag length: [16]\n",
      "Gold Classification tag ids length: [16]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Samantha : <file_video> Evelyn : LOL Holly : Is SHE making that noise?? Samatha : Yes (＾▽＾) Holly : How possible?? : o Samantha : Idk, I'm also surprised!! Evelyn : xD\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Samatha is making a noise. Holly is surprised.\n",
      "Generated summary length: torch.Size([1, 14])\n",
      "Gold summary length: [17]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 13])\n",
      "Gold Classification tag length: [17]\n",
      "Gold Classification tag ids length: [17]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Theresa : have you been at Tom's new place? Luis : yes, it's nice Marion : He invited us for a dinner Adam : where is it? Marion : a bit outside the city Adam : where exactly? Marion : Fiesole Luis : very nice!\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Theresa and Luis have been to Tom's new place. They invited him for a dinner.\n",
      "Generated summary length: torch.Size([1, 23])\n",
      "Gold summary length: [14]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 22])\n",
      "Gold Classification tag length: [14]\n",
      "Gold Classification tag ids length: [14]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Jane : Hello Vegano Resto : Hello, how may I help you today? Jane : I would like to make a reservation. Jane : For 6 people, tonight around 20 : 00 Vegano Resto : Let me just check. Vegano Resto : Ah, I'm afraid that there is no room at 20 : 00. Vegano Resto : However, I could offer you a table for six at 18 : 30 or at 21 : 00 Vegano Resto : Would either of those times suit you? Jane : Oh dear. Jane : Let me just ask my friends. Vegano Resto : No problem. Jane : 21 : 00 will be ok. Vegano Resto : Perfect. So tonight at 21 : 00 for six people under your name. Jane : great, thank you!\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Jane would like to make a reservation for six people at 18 : 30 or at 21 : 00. Vegano Resto will make the reservation at either time.\n",
      "Generated summary length: torch.Size([1, 35])\n",
      "Gold summary length: [14]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 36])\n",
      "Gold Classification tag length: [14]\n",
      "Gold Classification tag ids length: [14]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Nancy : Howdy, how y\\'all doin\\'? Tina : Is that a Texan drawl, girl? Nancy : Yes ma\\'am! Loving it out here! Tina : How\\'s the job going? Kids behaving themselves? Nancy : Mostly! They laugh at my accent though! Tina : Well, they probably haven\\'t met a Welsh person before! Nancy : No shit! They ask me to repeat everything! Best one is \"Water\", course, it\\'s mostly \"Waarderr\" here! Tina : LOL. I\\'d love to hear that, you picked up the accent yet? Nancy : Nah, 21 years in Cardiff isn\\'t easily removed! Tina : We\\'re missing you here, the pub is quiet these days without your laugh! Nancy : Miss you too! I\\'m coming home in 6 weeks, though. Last fortnight I\\'m going travelling with 3 other Brits working here, a Geordie girl, a guy from Belfast and Annie, who\\'s from Glasgow. Tina : My God, I\\'m so jealous! I bet they had even more trouble being understood out there! See you after your trip!']\n",
      "Generated Summaries:\n",
      "\n",
      " Nancy is working at a nursery in Cardiff. She has a Texan accent. She is going home in 6 weeks. Tina is going travelling with 3 other Brits working here.\n",
      "Generated summary length: torch.Size([1, 39])\n",
      "Gold summary length: [33]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 38])\n",
      "Gold Classification tag length: [33]\n",
      "Gold Classification tag ids length: [33]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Laura : I need a new printer : / Laura : thinking about this one Laura : <file_other> Jamie : you're sure you need a new one? Jamie : I mean you can buy a second hand one Laura : could be\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Laura is thinking about getting a new printer. Jamie wants to buy a second hand one.\n",
      "Generated summary length: torch.Size([1, 21])\n",
      "Gold summary length: [8]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 20])\n",
      "Gold Classification tag length: [8]\n",
      "Gold Classification tag ids length: [8]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Barbara : got everything? Haylee : yeah almost Haylee : i'm in dairy section Haylee : but can't find this youghurt u wanted Barbara : the coconut milk one? Haylee : yeah Barbara : hmmm yeah that's a mystery. cause it's not dairy but it's yoghurt xD Haylee : exactly xD Haylee : ok i asked sb. they put it next to eggs lol Barbara : lol\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Haylee is in the dairy section but she can't find the coconut milk one she wanted.\n",
      "Generated summary length: torch.Size([1, 22])\n",
      "Gold summary length: [8]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 27])\n",
      "Gold Classification tag length: [8]\n",
      "Gold Classification tag ids length: [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Norbert : we need to hurry to catch the tour Wendy : ok, am buying something. be right out! Norbert : ok. am not waiting long though. missed the last one because of you Wendy : just be patient for once. Norbert : im always patient Wendy : at the register now Norbert : alright']\n",
      "Generated Summaries:\n",
      "\n",
      " Norbert and Wendy are buying something. They need to hurry to catch the tour.\n",
      "Generated summary length: torch.Size([1, 20])\n",
      "Gold summary length: [15]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 21])\n",
      "Gold Classification tag length: [15]\n",
      "Gold Classification tag ids length: [15]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Lidia : hi guys, how was your day? Cecil : amazing Lidia : where did you go? Cheryl : to the Jandia Peninsula Cheryl : sorry, Cecil is driving Lidia : and how was it? Cheryl : I liked it a lot Cheryl : Peter took very nice pics Peter : <file_photo> <file_photo> Peter : but it was very windy Lidia : yes, it's always windy here Peter : really? Also in summer? Lidia : sure, the name Fuerteventura means strong wind Cheryl : wow, it's fascinating Lidia : so do you have any plans for tomorrow Cheryl : Cecil wants to explore more the south of the island Peter : I'm just a passenger, so have no voice Cheryl : c'mon, it's not true Peter : I'm just joking Cheryl : we will decide after dinner Cecil : ok, so let me know Cheryl : we will\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Lidia and Cecil went to Fuerteventura yesterday. They went to the Jandia Peninsula in the south of the island. It was very windy.\n",
      "Generated summary length: torch.Size([1, 38])\n",
      "Gold summary length: [34]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 37])\n",
      "Gold Classification tag length: [34]\n",
      "Gold Classification tag ids length: [34]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Nickola : Have you found it? Sophie : No! Still looking : ( Nickola : Check pockets and handbags. Sophie : Checked them all twice already...']\n",
      "Generated Summaries:\n",
      "\n",
      " Sophie is still looking for Nickola's purse.\n",
      "Generated summary length: torch.Size([1, 13])\n",
      "Gold summary length: [12]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 21])\n",
      "Gold Classification tag length: [12]\n",
      "Gold Classification tag ids length: [12]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [' Rosie : What\\'s your favorite b-movie? Elle : um, hard to say. Why do you ask? Dennis : Toxic avenger for sure Rosie : I have to write an essay and I chose bad movies as my topic and I\\'m just looking for inspiration Elle : plan 9 from outer space is definitely something worth mentioning Rosie : Yeah, I\\'ve seen it. And I will also cover \"The Room\". I\\'m just looking for something a bit more niche Dennis : There\\'s Troma Studio for ya - toxic avenger, poultrygeist - the latter is exceptionally awful - and it\\'s a musical Rosie : Is it one of those intentionally bad movies? Dennis : most definitely Rosie : ok, thank you, I\\'ll check it out Elle : oh, there\\'s also jesus christ vampire hunter Rosie : what? : D Elle : it\\'s even worse than it sounds Dennis : and when it comes to more recent movies there are those stupid animal-based horror movies like sharknado or zombeavers Rosie : I\\'ve heard of sharknado and zombeavers sound just awesome Rosie : thanks guys, you helped me a lot : )']\n",
      "Generated Summaries:\n",
      "\n",
      " Rosie has to write an essay about bad movies. She wants to cover \"The Room\" and \"The Toxic Avenger\". She will also cover \"Zombeavers\".\n",
      "Generated summary length: torch.Size([1, 39])\n",
      "Gold summary length: [14]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 48])\n",
      "Gold Classification tag length: [14]\n",
      "Gold Classification tag ids length: [14]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Julia : What is your biggest dream Julia : I mean the kind that can be achieved James : Everyone say I have nice voice James : My mom liked very much when I was reading outloud James : I've had this dream for some time now, to become a voice actor James : Be a part of cartoon or video game as a voice actor reading a character Julia : Wow. Nice one. Julia : Btw you do have a nice voice Julia : I could listen to you as a radio speaker. James : Thanks James : I've worked in radio, but it was during college so I had little time for this Julia : Shame. James : I know. But nothing is lost. I still have microphone at home and with a bit of help I could make homemade radio station Julia : That's actually a great idea Julia : I cheer for you!\"]\n",
      "Generated Summaries:\n",
      "\n",
      " James wants to become a voice actor. He wants to be a part of a cartoon or video game. He has a good voice and would like to make a radio station.\n",
      "Generated summary length: torch.Size([1, 39])\n",
      "Gold summary length: [18]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 57])\n",
      "Gold Classification tag length: [18]\n",
      "Gold Classification tag ids length: [18]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Poppy : I literally cannot think any more today! Alice : Yeah, I'm in the same shape. What a long day! Poppy : Lunch went by in a flash because I had errands, which makes the day so slow! Alice : I didn't get lunch, so that's even worse! Poppy : Oh, poor you! Aren't you starving? Alice : I'll live. Only three more hours! Poppy : LOL! Not that you're counting... Alice : Damn straight I'm counting! LOL! Poppy : Well, I'm def going for drinks after work. Want to join? Alice : Who else is there? Poppy : Nobody yet but I was going to put the word out. Alice : Sure, sounds fun. I'll invite some people up here if that's okay? Poppy : Oh, got your eye on anyone? Alice : Fred! Poppy : Fred? Really? Alice : Sure, why not? He's single, my age and not bad looking. Poppy : He's a dork! Alice : But a cute one! Poppy : If you say so. Not my type! Alice : That's a relief! Poppy : He's all yours! Alice :\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Alice and Poppy are going for drinks after work. They are going to meet up with Fred for drinks. They have to go out for three hours. Alice didn't get lunch because she had errands.\n",
      "Generated summary length: torch.Size([1, 47])\n",
      "Gold summary length: [31]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 51])\n",
      "Gold Classification tag length: [31]\n",
      "Gold Classification tag ids length: [31]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Sash : need to see u Caron : y Caron : i'm out from 12 Sash : will be before Sash : then Caron : k Sash : open the door : Caron : what time u coming I need to go out Sash : soon Caron : hurry up I need to go out\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Sash will see Caron at 12. Caron will be out from 12.\n",
      "Generated summary length: torch.Size([1, 20])\n",
      "Gold summary length: [11]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 22])\n",
      "Gold Classification tag length: [11]\n",
      "Gold Classification tag ids length: [11]\n",
      "-----------------------------------------------------------------------------------------\n",
      "dialog:  [\" Giuseppe : Hi man Matteo : Yo Giuseppe : How's it going with Gosia? Matteo : I don't know, she's a little strange Giuseppe : Why? Matteo : She always criticizes me because I like football and video games Giuseppe : Damn Matteo : Yeah... Giuseppe : Ok, I don't like games either, but... Matteo : You boring guy Giuseppe : Lol Matteo : Anyway I like her a lot Giuseppe : I can understand that, she's hot, if you ever dump her make sure you tell me Matteo : Get your hands off her, man Giuseppe : Just kidding Matteo : Lollolol\"]\n",
      "Generated Summaries:\n",
      "\n",
      " Matteo likes Gosia, but she criticizes him because he likes football and video games.\n",
      "Generated summary length: torch.Size([1, 22])\n",
      "Gold summary length: [15]\n",
      "Generated Classification Labels:\n",
      "\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Generated classification tag length: torch.Size([1, 26])\n",
      "Gold Classification tag length: [15]\n",
      "Gold Classification tag ids length: [15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary:  A wants to take her son to the animal shelter to get a puppy for him. She wants to get him one of those little dogs. She took him to the shelter last Monday and he really liked one of the dogs.\n",
      "Gold summary:  A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=5.848726259469986, metrics={'train_runtime': 368.7167, 'train_samples_per_second': 0.542, 'train_steps_per_second': 0.542, 'total_flos': 108358064025600.0, 'train_loss': 5.848726259469986, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a8a5a35-5a21-428f-9bed-675645651efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 28 02:37:37 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.13                 Driver Version: 537.13       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 Ti   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   43C    P2              34W / 310W |   7937MiB /  8192MiB |     49%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      6404    C+G   ...6.0_x64__cv1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8252    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      8500    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9496    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9596    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     11540    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13008    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     13552    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     14228    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14496    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     15156    C+G   ...12.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     16172      C   ...\\anaconda3\\envs\\cap_proj\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     19052      C   ...\\anaconda3\\envs\\cap_proj\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     19208      C   ...\\anaconda3\\envs\\cap_proj\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     20580    C+G   ...on\\119.0.2151.72\\msedgewebview2.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b088bdf3-6ba2-4198-b1a3-91dfa65018c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'summary_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate(tokenized_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cap_proj\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:165\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cap_proj\\Lib\\site-packages\\transformers\\trainer.py:3066\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3063\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3065\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3066\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3067\u001b[0m     eval_dataloader,\n\u001b[0;32m   3068\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3069\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[0;32m   3070\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[0;32m   3071\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3072\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[0;32m   3073\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[0;32m   3074\u001b[0m )\n\u001b[0;32m   3076\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cap_proj\\Lib\\site-packages\\transformers\\trainer.py:3255\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3252\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   3254\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 3255\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys)\n\u001b[0;32m   3256\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3257\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m, in \u001b[0;36mMySeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_step\u001b[39m(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      4\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     ignore_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[\u001b[38;5;28mfloat\u001b[39m], Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Perform an evaluation step on `model` using `inputs`.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Subclass and override to inject custom behavior.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m        labels (each being optional).\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     summary_len \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     26\u001b[0m     tag_ids_len \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags_ids_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     27\u001b[0m     tags_len \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cap_proj\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:253\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mwith the constraint of slice.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[item]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'summary_len'"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7452b49-20dd-402e-927a-9bce70e4fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"summarizer_w_classifier_loss_frozen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
