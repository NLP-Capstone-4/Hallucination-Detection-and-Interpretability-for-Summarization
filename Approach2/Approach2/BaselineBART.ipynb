{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfb3f14-7161-4734-a832-3a0762d481d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import re, json\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_metric,Dataset,DatasetDict, load_dataset, Sequence, Value\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, BartForConditionalGeneration\n",
    "from transformers import AutoTokenizer, Trainer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c05fd6-23ac-4055-845e-fd2c4fdb835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_numpy_rng = np.random.default_rng(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce90aa0-7033-427e-bc29-ee148a1acc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65402bdf-8983-45b7-b22a-8a576edf5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9af4c39-926f-407f-893b-346339dba231",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large\"\n",
    "metric = evaluate.load(\"rouge\")\n",
    "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4251206d-463c-42a1-827e-32cb25c25ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('samsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a31e95-f76f-428d-b5e1-2cc00e7b6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    inputs = [doc for doc in examples['dialogue']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors='pt', padding=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_inputs = tokenizer(examples[\"summary\"], truncation=True, return_tensors='pt', padding=True)\n",
    "        \n",
    "    model_inputs['labels'] = tokenized_inputs['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95cf1276-e3fe-4cb8-9909-c1f813346061",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af46311-0248-4620-bab0-c46d17bb4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(['id','dialogue', 'summary'])\n",
    "tokenized_datasets['validation'] = tokenized_datasets['validation'].remove_columns(['id','dialogue', 'summary'])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns(['id','dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87ebd78-55ab-41a5-8086-cb2865a49e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=4,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=42,\n",
    "    generation_max_length=max_target_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "076293cd-d640-4f71-ac7c-6b5ec2147236",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7231598e-f79c-431d-9e52-e0e437f543e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Generated summary: {decoded_preds[0]}\")\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Gold summary: {decoded_labels[0]}\")\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # print(result)\n",
    "    # result = {\"rouge\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8a2f89-e2fc-4efa-9878-3280d7f31f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e1dbddf-e03f-4669-87a7-9d0557b74176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2310/2310 32:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.567350</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>26.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.501947</td>\n",
       "      <td>0.525600</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>28.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>0.497809</td>\n",
       "      <td>0.522700</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>28.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.514800</td>\n",
       "      <td>0.491243</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>27.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.496187</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>28.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.495583</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.441100</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>26.596600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.499362</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>0.291400</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>27.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>0.500727</td>\n",
       "      <td>0.529500</td>\n",
       "      <td>0.287100</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>29.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>0.503964</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>29.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.504823</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.439400</td>\n",
       "      <td>28.865500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary: A wants to get a puppy for her son. She took him to the animal shelter last Monday. He showed her one that he really liked. He wanted to take it home right away.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. A took him to the animal shelter last Monday and he really liked it. A will get him one of those little dogs.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B will go with her to the animal shelter tomorrow afternoon. A took her son to the shelter last Monday and he liked the puppy. A will get him one of those little dogs. \n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B will take him to the animal shelter tomorrow afternoon.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B will take him to the animal shelter tomorrow afternoon.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B will take him to the animal shelter tomorrow afternoon.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B will take him to the animal shelter tomorrow afternoon.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B agrees to take him to the animal shelter tomorrow afternoon.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B will take him to the animal shelter tomorrow afternoon.\n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "Generated summary: A wants to get a puppy for her son. B will go with her to the animal shelter tomorrow afternoon. A took her son there last Monday and he really liked the puppy. \n",
      "Gold summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2310, training_loss=0.7229478450951639, metrics={'train_runtime': 1943.9178, 'train_samples_per_second': 75.785, 'train_steps_per_second': 1.188, 'total_flos': 7.981446249578496e+16, 'train_loss': 0.7229478450951639, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eca231a-22bd-4a8c-9e0e-c8f3d6c80d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary: Hannah doesn't have Betty's number. She doesn't know him well, but he called her last time they were at the park together. She will text him.\n",
      "Gold summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5453633666038513,\n",
       " 'eval_rouge1': 0.4821,\n",
       " 'eval_rouge2': 0.239,\n",
       " 'eval_rougeL': 0.3989,\n",
       " 'eval_rougeLsum': 0.3983,\n",
       " 'eval_gen_len': 25.6606,\n",
       " 'eval_runtime': 69.0753,\n",
       " 'eval_samples_per_second': 11.857,\n",
       " 'eval_steps_per_second': 0.188,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8a5a35-5a21-428f-9bed-675645651efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 15 16:16:49 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:A1:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              63W / 300W |  20016MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1841      C   /home/hjia71/miniconda3/bin/python        15242MiB |\n",
      "|    0   N/A  N/A     18055      C   .../miniconda3/envs/nlp_env/bin/python     2456MiB |\n",
      "|    0   N/A  N/A     18803      C   .../miniconda3/envs/lrl_env/bin/python     2294MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc4a6f-6fed-4a26-bb51-6296b6bfa164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
